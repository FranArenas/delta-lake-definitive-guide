{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb095ad4-fb4e-4284-8bc1-da99f514853d",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "## Essential Operations\n",
    "\n",
    "This is a secondary notebook dedicated mostly to using the DeltaTable API.\n",
    "\n",
    "This depends on previously running the first chapter 3 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca73c0de-a1c0-423b-8a50-90e42d146d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6dd822-f3ef-42ff-8517-ea32b8859758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, country: string, capital: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DeltaTable object to interact with\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"countries.delta\")\n",
    "delta_table.toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a35222-751b-4143-9a54-52f5c833735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      9|2024-05-29 12:57:...|  NULL|    NULL|       MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          8|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.5....|\n",
      "|      8|2024-05-29 12:55:...|  NULL|    NULL|      DELETE|{predicate -> [\"(...|NULL|    NULL|     NULL|          7|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      7|2024-05-29 12:55:...|  NULL|    NULL|     RESTORE|{version -> 1, ti...|NULL|    NULL|     NULL|          6|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      6|2024-05-29 12:52:...|  NULL|    NULL|      DELETE|{predicate -> [\"(...|NULL|    NULL|     NULL|          5|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      5|2024-05-29 12:52:...|  NULL|    NULL|     RESTORE|{version -> 1, ti...|NULL|    NULL|     NULL|          4|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      4|2024-05-29 12:51:...|  NULL|    NULL|      DELETE|{predicate -> [\"(...|NULL|    NULL|     NULL|          3|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      3|2024-05-29 12:49:...|  NULL|    NULL|     RESTORE|{version -> 1, ti...|NULL|    NULL|     NULL|          2|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      2|2024-05-29 12:41:...|  NULL|    NULL|       WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2024-05-29 12:38:...|  NULL|    NULL|       WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2024-05-29 12:30:...|  NULL|    NULL|CREATE TABLE|{isManaged -> fal...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|                  {}|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the table history\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8092fd29-ad2b-4f3a-9237-206db55f8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a specific timestamp you can restore time specific versions\n",
    "# delta_table.restoreToTimestamp('2024-05-24').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6256fecf-ce7a-4674-b6c4-1b71dbb3ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also restore by version number (easier in this case since we just made the tables)\n",
    "delta_table.restoreToVersion(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "815cff9c-a70f-4566-ba09-fd89f853fd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Pyspark you can use time travel to read from the table, update the value accordingly\n",
    "spark.read.option(\"timestampAsOf\", \"2024-05-29 12:31:00\").load(\"countries.delta\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14296403-cd1a-42ad-9a2b-10907f657a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pyspark time travel by version number\n",
    "spark.read.option(\"versionAsOf\", \"1\").load(\"countries.delta\").select(\"id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d540a8c6-ef4d-4823-9928-be3dfb7e216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "| id|country|capital|\n",
      "+---+-------+-------+\n",
      "|  2| Canada|Toronto|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delteing values with Pyspark functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "delta_table.delete(col(\"id\") == 1)\n",
    "df = delta_table.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15547ca1-4557-4ba4-a845-bedb502de45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# permissions error? works correctly though\n",
    "(\n",
    "spark\n",
    ".createDataFrame(\n",
    "    [\n",
    "    (1, 'India', 'New Delhi'),\n",
    "    (4, 'Australia', 'Canberra')\n",
    "    ],\n",
    "    schema=[\"id\", \"country\", \"capital\"]\n",
    "    )\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"exampleDB.delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd8d1af-7638-4c6c-9052-2d0e8180d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working right with the derby metastore but if you need to replace with the DeltaTable API this is how it is done\n",
    "# delta_table2 = (\n",
    "#     DeltaTable.replace(spark)\n",
    "#     .tableName(\"countries.delta\")\n",
    "#     .addColumns(df.schema)\n",
    "#     .execute()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092ddc78-4621-470e-be55-e966ed791a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mergeBuilder to upsert values\n",
    "idf = (\n",
    "    spark\n",
    "    .createDataFrame([\n",
    "        (1, 'India', 'New Delhi'),\n",
    "        (4, 'Australia', 'Canberra')],\n",
    "        schema=[\"id\", \"country\", \"capital\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source = idf.alias(\"source\"),\n",
    "    condition = \"source.id = target.id\"\n",
    "  ).whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"country\": \"source.country\",\n",
    "      \"capital\": \"source.capital\"\n",
    "    }\n",
    "  ).whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"id\": \"source.id\",\n",
    "      \"country\": \"source.country\",\n",
    "      \"capital\": \"source.capital\"\n",
    "    }\n",
    "  ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "056b9592-9833-499f-b2a6-e49fb1f1b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|  country|  capital|\n",
      "+---+---------+---------+\n",
      "|  4|Australia| Canberra|\n",
      "|  1|    India|New Delhi|\n",
      "|  2|   Canada|  Toronto|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the final results\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0ef57-01fe-42dc-8cf4-6a23b854d863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
