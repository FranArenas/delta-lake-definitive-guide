{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709b742c-ac79-44e1-a6ab-bf361b0b2c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seedir in /home/NBuser/.local/lib/python3.9/site-packages (0.4.2)\n",
      "Requirement already satisfied: natsort in /home/NBuser/.local/lib/python3.9/site-packages (from seedir) (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --disable-pip-version-check seedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8bbb43-0aa9-402b-8c17-c52420557872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b156a23-e5fa-4487-a4c2-c5347cd92ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create some resources\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "try:\n",
    "    os.chdir(\"/opt/spark/work-dir/ch12/\")\n",
    "except FileNotFoundError:\n",
    "    raise\n",
    "\n",
    "# Remove old data if exists\n",
    "try:\n",
    "    subprocess.run([\"rm\", \"-rf\", \"/tmp/delta/partitioning.example.delta/\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.createHiveTableByDefault\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d5705a-a78b-407d-b9c6-21f92cf6d7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deltalake.writer import write_deltalake\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=[\n",
    "    (1, \"Customer 1\", \"free\"),\n",
    "    (2, \"Customer 2\", \"paid\"),\n",
    "    (3, \"Customer 3\", \"free\"),\n",
    "    (4, \"Customer 4\", \"paid\")],\n",
    "    columns=[\"id\", \"name\", \"membership_type\"])\n",
    "\n",
    "write_deltalake(\n",
    "  \"/tmp/delta/partitioning.example.delta\",\n",
    "  data=df,\n",
    "  mode=\"overwrite\",\n",
    "  partition_by=[\"membership_type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133cb2ac-c9bf-4af9-9664-dededba208a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitioning.example.delta/\n",
      "├─_delta_log/\n",
      "│ └─00000000000000000000.json\n",
      "├─membership_type=paid/\n",
      "│ └─0-cb247011-b520-472c-af04-384138d2fdfb-0.parquet\n",
      "└─membership_type=free/\n",
      "  └─0-cb247011-b520-472c-af04-384138d2fdfb-0.parquet\n"
     ]
    }
   ],
   "source": [
    "from seedir import seedir\n",
    "seedir(\"/tmp/delta/partitioning.example.delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451b7230-9034-436f-9715-e2e256c121b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"delta.autoCompact.maxFileSize\", \"32mb\")\n",
    "spark.conf.set(\"delta.autoCompact.minNumFiles\", \"1\")\n",
    "spark.conf.set(\"delta.autoCompact.target\", \"commit\")\n",
    "spark.conf.set(\"delta.optimizeWrites\", \"true\")\n",
    "spark.conf.set(\"delta.targetFileSize\", \"24mb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2945cc-3506-4865-b824-a4f70ede31b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"numRecords\": 2, \"minValues\": {\"id\": 1, \"name\": \"Customer 1\"}, \"maxValues\": {\"id\": 3, \"name\": \"Customer 3\"}, \"nullCount\": {\"id\": 0, \"name\": 0}}\n",
      "{\"numRecords\": 2, \"minValues\": {\"id\": 2, \"name\": \"Customer 2\"}, \"maxValues\": {\"id\": 4, \"name\": \"Customer 4\"}, \"nullCount\": {\"id\": 0, \"name\": 0}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "basepath = \"/tmp/delta/partitioning.example.delta/\"\n",
    "fname = basepath + \"_delta_log/00000000000000000000.json\"\n",
    "with open(fname) as f:\n",
    "    for i in f.readlines():\n",
    "        parsed = json.loads(i)\n",
    "        if 'add' in parsed.keys():\n",
    "            stats = json.loads(parsed['add']['stats'])\n",
    "            print(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f87d45f-a3b3-403a-b875-6b80f806f1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [max(id#33L) AS max(id)#37L], Statistics(sizeInBytes=16.0 B, rowCount=1)\n",
      "+- Project [id#33L], Statistics(sizeInBytes=1460.0 B)\n",
      "   +- Relation [id#33L,name#34,membership_type#35] parquet, Statistics(sizeInBytes=5.0 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[max(id#33L)], output=[max(id)#37L])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=153]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_max(id#33L)], output=[max#616L])\n",
      "      +- *(1) Project [id#33L]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [id#33L,membership_type#35] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/tmp/delta/partitioning.example.delta], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe in the logical plan that we only need to check table statistics to find the value for a column max\n",
    "spark.sql(\"select max(id) from delta.`/tmp/delta/partitioning.example.delta`\").explain(\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee70530-2150-42f2-b49b-9bc71a536a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    delta.`/tmp/delta/partitioning.example.delta`\n",
    "    set tblproperties(\"delta.dataSkippingNumIndexedCols\"=5)\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    delta.`/tmp/delta/partitioning.example.delta`\n",
    "    CHANGE id first;\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    delta.`/tmp/delta/partitioning.example.delta`\n",
    "    CHANGE name after membership_type;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf971553-8d1a-45ad-81ee-c2d0860c0c43",
   "metadata": {},
   "source": [
    "# Cluster By Example\n",
    "(only works on Databricks right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1596245-e3c4-4be1-b152-9e7c06e41249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must run this example on a Databricks Spark runtime\n"
     ]
    }
   ],
   "source": [
    "# interrupt execution in non-Databricks environment\n",
    "try:\n",
    "    assert(\"Databricks\" in spark.conf.get(\"spark.app.name\"))\n",
    "except:\n",
    "    print(\"You must run this example on a Databricks Spark runtime\")\n",
    "    class StopExecution(Exception):\n",
    "        def _render_traceback_(self):\n",
    "            pass\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e417c37-99a0-4f2f-a791-3426c7f3e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles _path = (\"/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")\n",
    "\n",
    "parquetDf = (\n",
    "    spark\n",
    "    .read\n",
    "    .parquet(articles_path)\n",
    "    )\n",
    "parquetDf.createOrReplaceTempView(\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3aeec4-a9f6-46c6-84e6-6cbf8fb3e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table example.wikipages\n",
    "cluster by (id)\n",
    "as (select *,date(revisionTimestamp) as articleDate from source)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ca02c-12f3-4034-8f6c-ad9b0e077d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    set tblproperties (\"delta.dataSkippingNumIndexedCols\"=5);\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    CHANGE articleDate first;\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    CHANGE `text` after revisionTimestamp;\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    CLUSTER BY (articleDate);\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d4674-6c23-447f-b557-acdcfc17876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"OPTIMIZE example.wikipages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585f9ef-760d-4aca-a87d-feab7908c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "  year(articleDate) as PublishingYear,\n",
    "  count(distinct title) as Articles\n",
    "from\n",
    "  example.wikipages\n",
    "where\n",
    "  month(articleDate)=3\n",
    "and\n",
    "  day(articleDate)=4\n",
    "group by\n",
    "  year(articleDate)\n",
    "order by\n",
    "  publishingYear\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85a6e6-1efd-4714-b171-3421feee0056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "cdf = spark.table(\"example.wikipages\")\n",
    "raw_items = cdf.agg(countDistinct(cdf.id)).collect()[0][0]\n",
    "num_items = int(raw_items * 1.25)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "create bloomfilter index\n",
    "on table\n",
    "example.wikipages\n",
    "for columns\n",
    "(id options (fpp=0.05, numItems={num_items}))\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
