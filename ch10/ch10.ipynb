{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5824781d-e853-4e63-8163-9fef038d51a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chapter 10: Advanced Features\n",
    "\n",
    "## Deletion Vectors Overview\n",
    "\n",
    "### The Main Idea\n",
    "Sometimes we can look at a problem and think of different ways to solve it. One of the ways we've done this in Delta Lake is to enable the feature called [deletion vectors](https://docs.delta.io/latest/delta-deletion-vectors.html). One of the things you will should consider in data design thinking about how you want to optimize the performance of a table. In essence, you can optimize for either the table readers or the table writers. It's also important to note that the term deletion vector defines more of the form and function of what the process does rather than how it helps you as a feature. One of the key benefits is that it gives you the ability to do a *merge-on-read* operation. It drastically reduces the performance implications of doing simple, delete operations and instead postpones the performance impact of those delete operations until a more convenient time in the future (but in a rather efficient manner).\n",
    "\n",
    "### Merge-on-Read Explained\n",
    "What does merge-on-read mean? Merge-on-read means that instead of going through the operation of rewriting a file at the time of deleting a record or set of records from a particular file we instead make some kind of a note that those records are deleted, and postpone the performance impact of actually performing the delete operation until a time where we can run and optimize command or more complicated update statement. Of course, if someone were to read the table after a merge-on-read operation has been initiated then it will merge during that read operation and that's kind of the point because it allows us to minimize the performance impacts of performing a simple delete operation, or even multiple delete operations, and just perform it at a time later where you are already filtering on the same set of files while reading them and otherwise avoid it doing so in situation's where you don't need it to happen straight away.\n",
    "Deletion vectors are a way to get this kind of merge-on-read behavior. Put simply, deletion vectors are just a file (or multiple files) adjacent to a data file that allows you to know which records are to be deleted out of the larger data file it sits beside and push that delete (rewrite) operation off to a later point in time that is more efficient and convenient). Alongside in this case is relative, it is part of the Delta Lake file but you will notice in partitioned tables that the deletion vector files sit at the top level rather than within the partition. You can observe this in the coming example.\n",
    "\n",
    "### Working Data\n",
    "Our data originally hails from [this repo](https://github.com/nytimes/covid-19-data/issues) but is available in [the repo for this docker image](https://github.com/delta-io/delta-docker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3301b1d7-1fe3-49c7-b4c8-3e8ae99e36ef",
     "showTitle": true,
     "title": "Clear table destination"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf nyt_covid_19/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists nyt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bebf9d38-83ff-44be-ade8-ad9ca98e6bf5",
     "showTitle": true,
     "title": "Create working table"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "(\n",
    "spark\n",
    " .read\n",
    " .load(\"rs/data/COVID-19_NYT/\")\n",
    " .filter(col(\"state\")==\"Florida\")\n",
    " .filter(col(\"county\").isin(['Hillsborough', 'Pasco', 'Pinellas', 'Sarasota']))\n",
    " .repartition(\"county\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .partitionBy(\"county\")\n",
    " .option(\"path\", \"nyt_covid_19/\")\n",
    " .save()\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "spark\n",
    ".read\n",
    ".load(\"nyt_covid_19/\")\n",
    ".write\n",
    ".partitionBy(\"county\")\n",
    ".mode(\"overwrite\")\n",
    ".format(\"delta\")\n",
    ".saveAsTable(\"nyt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68676329-ae7b-46d4-b23a-1071f5e6d7cd",
     "showTitle": true,
     "title": "Check number of records with a set of filters"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+---------+\n",
      "|      date|  county|  state|rec_count|\n",
      "+----------+--------+-------+---------+\n",
      "|2020-03-11|Pinellas|Florida|        1|\n",
      "+----------+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "  date,\n",
    "  county,\n",
    "  state,\n",
    "  count(1) as rec_count\n",
    "from\n",
    "  nyt\n",
    "where\n",
    "  county=\"Pinellas\"\n",
    "and\n",
    "  date=\"2020-03-11\"\n",
    "group by\n",
    "  date,\n",
    "  county,\n",
    "  state\n",
    "order by\n",
    "  date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b875a051-f1ea-4e49-8b04-b3950d7ba15c",
     "showTitle": true,
     "title": "Enable DV feature (raises table versions)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE nyt SET TBLPROPERTIES ('delta.enableDeletionVectors' = true);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034ce579-fbec-4d42-8246-c35269628067",
     "showTitle": true,
     "title": "Check table layout as created"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mspark-warehouse/nyt/\u001b[0m\n",
      "├── \u001b[01;34mcounty=Hillsborough\u001b[0m\n",
      "│   └── \u001b[00mpart-00000-751576f9-bacd-4eaa-b747-c3cd64aceacd.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pasco\u001b[0m\n",
      "│   └── \u001b[00mpart-00003-37880ee0-2ae8-4aa3-b031-408456d9fbc4.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pinellas\u001b[0m\n",
      "│   └── \u001b[00mpart-00001-280a8daf-82fd-4fb5-95a5-43928a15597c.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Sarasota\u001b[0m\n",
      "│   └── \u001b[00mpart-00002-5b8a4fda-70ad-491d-b9a8-4581d1fc7b40.c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[01;34m_delta_log\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "    └── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "\n",
      "5 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree spark-warehouse/nyt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8f7088-169b-4d26-8106-2cd3146bda94",
     "showTitle": true,
     "title": "Run delete on record level filters"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "delete\n",
    "from\n",
    "  nyt\n",
    "where\n",
    "  county='Pinellas'\n",
    "and\n",
    "  date='2020-03-11'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df57615b-07cd-48a1-ad1e-2707d4cda188",
     "showTitle": true,
     "title": "We get a new version but no file differences"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mspark-warehouse/nyt/\u001b[0m\n",
      "├── \u001b[01;34mcounty=Hillsborough\u001b[0m\n",
      "│   └── \u001b[00mpart-00000-751576f9-bacd-4eaa-b747-c3cd64aceacd.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pasco\u001b[0m\n",
      "│   └── \u001b[00mpart-00003-37880ee0-2ae8-4aa3-b031-408456d9fbc4.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pinellas\u001b[0m\n",
      "│   └── \u001b[00mpart-00001-280a8daf-82fd-4fb5-95a5-43928a15597c.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Sarasota\u001b[0m\n",
      "│   └── \u001b[00mpart-00002-5b8a4fda-70ad-491d-b9a8-4581d1fc7b40.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_177d0a71-acb5-4815-a54a-c998b3e0c6cc.bin\u001b[0m\n",
      "└── \u001b[01;34m_delta_log\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "    └── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "\n",
      "5 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree spark-warehouse/nyt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3888ed8-7a51-4f3d-8b18-08468ae9230e",
     "showTitle": true,
     "title": "Try deletion at partition level"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|              367|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "delete\n",
    "from\n",
    "  nyt\n",
    "where\n",
    "  county='Pasco'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549defdb-cead-451f-8663-d8104aa9d1ef",
     "showTitle": true,
     "title": "Larger delete crossing partition boundaries"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                3|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "delete\n",
    "from\n",
    "  nyt\n",
    "where\n",
    "  date='2020-03-13'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca8340e-d7bc-466a-9828-4cc85ba6c011",
     "showTitle": true,
     "title": "Verify we get a new DV"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mspark-warehouse/nyt/\u001b[0m\n",
      "├── \u001b[01;34mcounty=Hillsborough\u001b[0m\n",
      "│   └── \u001b[00mpart-00000-751576f9-bacd-4eaa-b747-c3cd64aceacd.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pasco\u001b[0m\n",
      "│   └── \u001b[00mpart-00003-37880ee0-2ae8-4aa3-b031-408456d9fbc4.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pinellas\u001b[0m\n",
      "│   └── \u001b[00mpart-00001-280a8daf-82fd-4fb5-95a5-43928a15597c.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Sarasota\u001b[0m\n",
      "│   └── \u001b[00mpart-00002-5b8a4fda-70ad-491d-b9a8-4581d1fc7b40.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_177d0a71-acb5-4815-a54a-c998b3e0c6cc.bin\u001b[0m\n",
      "├── \u001b[00mdeletion_vector_5019afe7-8cf6-48cc-85ce-7a0ace4cc554.bin\u001b[0m\n",
      "└── \u001b[01;34m_delta_log\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000003.json\u001b[0m\n",
      "    └── \u001b[00m00000000000000000004.json\u001b[0m\n",
      "\n",
      "5 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!tree spark-warehouse/nyt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9663399-1186-4ceb-878b-fe473e4aae4c",
     "showTitle": true,
     "title": "Check the table history"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|operationMetrics                                                                                                                                                                                                                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 3, numDeletionVectorsRemoved -> 1, numAddedChangeFiles -> 0, executionTimeMs -> 458, numDeletionVectorsUpdated -> 1, numDeletedRows -> 3, scanTimeMs -> 0, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}       |\n",
      "|{numRemovedFiles -> 1, numRemovedBytes -> 5777, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 138, numDeletionVectorsUpdated -> 0, numDeletedRows -> 367, scanTimeMs -> 138, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}|\n",
      "|{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 722, numDeletionVectorsUpdated -> 0, numDeletedRows -> 1, scanTimeMs -> 0, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}       |\n",
      "|{}                                                                                                                                                                                                                                                                                                                       |\n",
      "|{numFiles -> 4, numOutputRows -> 1475, numOutputBytes -> 23908}                                                                                                                                                                                                                                                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe history nyt\").select(\"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d69a7898-fc34-4a37-86ab-83776e29148b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Notice that each one of our actions created a new table version, even when all we did was enable deletion vectors. If we look at the history results we can get a clearer picture of what happened at each step for the deletions. When we deleted just a single record from the table or multiple records from multiple files we can see in the operation metrics that we added a deletion vector to the table. When we deleted records aligned to entire partition the entire file was marked for removal and so we don't need a deletion vector to be added, it simply becomes stale and will be removed on the next clean up operation. When one of the deletion vectors was being applied to a table which already had a deletion vector being applied to it then it combines those results and marks the original for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61ea334-f2bb-456a-9f22-579d1caff301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|path                                        |metrics                                                                                                                                                      |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|file:/opt/spark/work-dir/spark-warehouse/nyt|{3, 3, {5882, 6307, 6030.0, 3, 18090}, {5893, 6317, 6043.666666666667, 3, 18131}, 3, NULL, 3, 3, 0, false, 0, 0, 1712841241751, 0, 12, 0, NULL, {3, 4}, 6, 6}|\n",
      "+--------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"optimize nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8252109-0484-439b-b0be-cd4e3c1ab24b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95745058-7928-45c7-9010-ac620743bb99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 6 files and directories in a total of 5 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"vacuum nyt retain 0 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff2fa3c-05dc-44d0-8987-a50cb13e7cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mspark-warehouse/nyt/\u001b[0m\n",
      "├── \u001b[01;34mcounty=Hillsborough\u001b[0m\n",
      "│   └── \u001b[00mpart-00000-4c40fe6f-f092-4ead-acea-600d67c80dc0.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pasco\u001b[0m\n",
      "├── \u001b[01;34mcounty=Pinellas\u001b[0m\n",
      "│   └── \u001b[00mpart-00000-2f40964d-695c-4621-8559-b4f81b459993.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mcounty=Sarasota\u001b[0m\n",
      "│   └── \u001b[00mpart-00000-2dc1a17b-19da-4f1b-826e-d71b91837d60.c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[01;34m_delta_log\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000001.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000002.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000003.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000004.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000005.json\u001b[0m\n",
      "    ├── \u001b[00m00000000000000000006.json\u001b[0m\n",
      "    └── \u001b[00m00000000000000000007.json\u001b[0m\n",
      "\n",
      "5 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!tree spark-warehouse/nyt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transaction comments for successive operations\n",
    "spark.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", \"setting-constraints-and-comments-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists example_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table example_table (\n",
    "  id int comment 'uid column',\n",
    "  content string not null comment 'payload column for text'\n",
    ")\n",
    "using delta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"alter table example_table alter column id comment 'unique id column'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE example_table ADD CONSTRAINT id CHECK (id > 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------------+\n",
      "|col_name|data_type|comment                |\n",
      "+--------+---------+-----------------------+\n",
      "|id      |int      |unique id column       |\n",
      "|content |string   |payload column for text|\n",
      "+--------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe example_table\").show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+--------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------+----------------------------------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation     |operationParameters                                                                                 |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata                            |engineInfo                         |\n",
      "+-------+-----------------------+------+--------+--------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------+----------------------------------------+-----------------------------------+\n",
      "|2      |2024-04-11 13:14:08.489|NULL  |NULL    |ADD CONSTRAINT|{name -> id, expr -> id > 0}                                                                        |NULL|NULL    |NULL     |1          |Serializable  |false        |{}              |setting-constraints-and-comments-example|Apache-Spark/3.5.1 Delta-Lake/3.1.0|\n",
      "|1      |2024-04-11 13:14:08.036|NULL  |NULL    |CHANGE COLUMN |{column -> {\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{\"comment\":\"unique id column\"}}}|NULL|NULL    |NULL     |0          |Serializable  |true         |{}              |setting-constraints-and-comments-example|Apache-Spark/3.5.1 Delta-Lake/3.1.0|\n",
      "|0      |2024-04-11 13:14:07.715|NULL  |NULL    |CREATE TABLE  |{isManaged -> true, description -> NULL, partitionBy -> [], properties -> {}}                       |NULL|NULL    |NULL     |NULL       |Serializable  |true         |{}              |setting-constraints-and-comments-example|Apache-Spark/3.5.1 Delta-Lake/3.1.0|\n",
      "+-------+-----------------------+------+--------+--------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------+----------------------------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe history example_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|key                   |value |\n",
      "+----------------------+------+\n",
      "|delta.constraints.id  |id > 0|\n",
      "|delta.minReaderVersion|1     |\n",
      "|delta.minWriterVersion|3     |\n",
      "+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tblproperties example_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove annotations to log for successive operations\n",
    "spark.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", \"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into example_table (id, content) values (1, 'thingmabob')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to test failing the constraint\n",
    "# spark.sql(\"insert into example_table (id, content) values (0, 'widget')\")\n",
    "# Fails with: [DELTA_VIOLATE_CONSTRAINT_WITH_VALUES] CHECK constraint id (id > 0) violated by row with values: - id : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to test failing constraint\n",
    "# spark.sql(\"insert into example_table (id, content) values (2, null)\")\n",
    "# Fails with: [DELTA_NOT_NULL_CONSTRAINT_VIOLATED] NOT NULL constraint violated for column: content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 880375545490145,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "delta_DV",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
