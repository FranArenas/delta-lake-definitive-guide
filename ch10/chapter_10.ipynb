{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a379d9d9-039e-497d-8a24-6abce5368647",
   "metadata": {},
   "source": [
    "# Chapter 10: Performance Tuning\n",
    "\n",
    "Contains the code examples from the chapter.\n",
    "\n",
    "Liquid Clustering was introduced in `Delta Lake 3.1.0`.\n",
    "\n",
    "For a first step we perform a little environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709b742c-ac79-44e1-a6ab-bf361b0b2c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --disable-pip-version-check seedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b156a23-e5fa-4487-a4c2-c5347cd92ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create some resources\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "try:\n",
    "    os.chdir(\"/opt/spark/work-dir/ch12/\")\n",
    "except FileNotFoundError:\n",
    "    raise\n",
    "\n",
    "# Remove old data if exists\n",
    "try:\n",
    "    subprocess.run([\"rm\", \"-rf\", \"/tmp/delta/partitioning.example.delta/\"])\n",
    "    subprocess.run([\"rm\", \"-rf\", \"metastore/\"])\n",
    "    subprocess.run([\"rm\", \"derby.log\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.createHiveTableByDefault\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d61df-ad7a-4b19-8e07-ea4283c985ea",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "We create an example table with Pandas and partition it by a membership_type column which has two distinct values. This results in two partition directories in the parquet which we see in the following directory tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d5705a-a78b-407d-b9c6-21f92cf6d7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deltalake.writer import write_deltalake\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=[\n",
    "    (1, \"Customer 1\", \"free\"),\n",
    "    (2, \"Customer 2\", \"paid\"),\n",
    "    (3, \"Customer 3\", \"free\"),\n",
    "    (4, \"Customer 4\", \"paid\")],\n",
    "    columns=[\"id\", \"name\", \"membership_type\"])\n",
    "\n",
    "write_deltalake(\n",
    "  \"/tmp/delta/partitioning.example.delta\",\n",
    "  data=df,\n",
    "  mode=\"overwrite\",\n",
    "  partition_by=[\"membership_type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133cb2ac-c9bf-4af9-9664-dededba208a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitioning.example.delta/\n",
      "├─_delta_log/\n",
      "│ └─00000000000000000000.json\n",
      "├─membership_type=paid/\n",
      "│ └─0-079ab161-1fd2-4fde-af3b-1522934d98b0-0.parquet\n",
      "└─membership_type=free/\n",
      "  └─0-079ab161-1fd2-4fde-af3b-1522934d98b0-0.parquet\n"
     ]
    }
   ],
   "source": [
    "from seedir import seedir\n",
    "seedir(\"/tmp/delta/partitioning.example.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f179c8-67a0-4fe8-b7b4-ff9870e73798",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "Some examples for setting values to the configuration options mentioned in the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451b7230-9034-436f-9715-e2e256c121b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"delta.autoCompact.maxFileSize\", \"32mb\")\n",
    "spark.conf.set(\"delta.autoCompact.minNumFiles\", \"1\")\n",
    "spark.conf.set(\"delta.autoCompact.target\", \"commit\")\n",
    "spark.conf.set(\"delta.optimizeWrites\", \"true\")\n",
    "spark.conf.set(\"delta.targetFileSize\", \"24mb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69fd06-4ea6-4d8c-ad7e-7ec0b817be05",
   "metadata": {},
   "source": [
    "## File Statistics\n",
    "\n",
    "Parsing the json log data from the Delta Lake table we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2945cc-3506-4865-b824-a4f70ede31b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"numRecords\": 2, \"minValues\": {\"id\": 2, \"name\": \"Customer 2\"}, \"maxValues\": {\"id\": 4, \"name\": \"Customer 4\"}, \"nullCount\": {\"id\": 0, \"name\": 0}}\n",
      "{\"numRecords\": 2, \"minValues\": {\"id\": 1, \"name\": \"Customer 1\"}, \"maxValues\": {\"id\": 3, \"name\": \"Customer 3\"}, \"nullCount\": {\"id\": 0, \"name\": 0}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "basepath = \"/tmp/delta/partitioning.example.delta/\"\n",
    "fname = basepath + \"_delta_log/00000000000000000000.json\"\n",
    "with open(fname) as f:\n",
    "    for i in f.readlines():\n",
    "        parsed = json.loads(i)\n",
    "        if 'add' in parsed.keys():\n",
    "            stats = json.loads(parsed['add']['stats'])\n",
    "            print(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f898ae1-7b95-4cf3-99c4-7f027921a868",
   "metadata": {
    "tags": []
   },
   "source": [
    "## File Skipping\n",
    "\n",
    "Observe how in the ***Optimized Logical Plan*** we can see where it notes that the query we submitted can be answered from the table statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f87d45f-a3b3-403a-b875-6b80f806f1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [max(id#33L) AS max(id)#37L], Statistics(sizeInBytes=16.0 B, rowCount=1)\n",
      "+- Project [id#33L], Statistics(sizeInBytes=1460.0 B)\n",
      "   +- Relation [id#33L,name#34,membership_type#35] parquet, Statistics(sizeInBytes=5.0 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[max(id#33L)], output=[max(id)#37L])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=153]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_max(id#33L)], output=[max#616L])\n",
      "      +- *(1) Project [id#33L]\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [id#33L,membership_type#35] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/tmp/delta/partitioning.example.delta], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observe in the logical plan that we only need to check table statistics to find the value for a column max\n",
    "spark.sql(\"select max(id) from delta.`/tmp/delta/partitioning.example.delta`\").explain(\"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99f864-7b05-4ee6-8c49-cea834e8ff3d",
   "metadata": {},
   "source": [
    "## Rearranging Columns and Controlling Stats Collection\n",
    "\n",
    "Here we use `alter table` statements to make a few changes to our table structure and behavior.\n",
    "1. We reduce the number of columns to collect statistics on to `5`.\n",
    "1. We move the `id` column to come first to make sure we collect statistics for it still.\n",
    "1. We move the `name` column to come after the `membership_type` column to avoid statistics collection. This has the most benefit for much larger column types like *array*, *struct*, *json*, or *string* in cases where they are fairly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee70530-2150-42f2-b49b-9bc71a536a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    delta.`/tmp/delta/partitioning.example.delta`\n",
    "    set tblproperties(\"delta.dataSkippingNumIndexedCols\"=5)\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    delta.`/tmp/delta/partitioning.example.delta`\n",
    "    CHANGE id first;\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    delta.`/tmp/delta/partitioning.example.delta`\n",
    "    CHANGE name after membership_type;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf971553-8d1a-45ad-81ee-c2d0860c0c43",
   "metadata": {},
   "source": [
    "# Cluster By Example\n",
    "(only works on Databricks right now, execution halted below otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1596245-e3c4-4be1-b152-9e7c06e41249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must run this example on a Databricks Spark runtime\n"
     ]
    }
   ],
   "source": [
    "# interrupt execution in non-Databricks environment\n",
    "try:\n",
    "    assert(\"Databricks\" in spark.conf.get(\"spark.app.name\"))\n",
    "except:\n",
    "    print(\"You must run this example on a Databricks Spark runtime\")\n",
    "    class StopExecution(Exception):\n",
    "        def _render_traceback_(self):\n",
    "            pass\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7f7fe-5ad4-4189-b0ac-08dceb3a69b4",
   "metadata": {},
   "source": [
    "First, create a source dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e417c37-99a0-4f2f-a791-3426c7f3e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles _path = (\"/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")\n",
    "\n",
    "parquetDf = (\n",
    "    spark\n",
    "    .read\n",
    "    .parquet(articles_path)\n",
    "    )\n",
    "parquetDf.createOrReplaceTempView(\"source\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9293d80-133c-4a0b-94d7-a8b141961f22",
   "metadata": {},
   "source": [
    "Then we create a liquid clustering enabled table by using the `cluster by` parameter during table creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3aeec4-a9f6-46c6-84e6-6cbf8fb3e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table example.wikipages\n",
    "cluster by (id)\n",
    "as (select *,date(revisionTimestamp) as articleDate from source)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ce11f-a80b-429a-9ed4-9906a35528a0",
   "metadata": {},
   "source": [
    "Next we'll use some of our earlier table property changes to make this more efficient, the `text` column contains entire articles which can be fairly large. We'll also change the clustering key from our original definition. Note that we could also choose to `cluster by NONE` which effectively disables the clustering behavior going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ca02c-12f3-4034-8f6c-ad9b0e077d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    set tblproperties (\"delta.dataSkippingNumIndexedCols\"=5);\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    CHANGE articleDate first;\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    CHANGE `text` after revisionTimestamp;\n",
    "    \"\"\")\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE\n",
    "    example.wikipages\n",
    "    CLUSTER BY (articleDate);\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab6b08-8cf9-4449-b615-53f04c143c3a",
   "metadata": {},
   "source": [
    "Last we'll run an `optimize` action which will trigger the clustering action and rewrite the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d4674-6c23-447f-b557-acdcfc17876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"OPTIMIZE example.wikipages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6034af2-fe1c-42df-9889-5543fb68ad0f",
   "metadata": {},
   "source": [
    "We can test out the table with a query like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585f9ef-760d-4aca-a87d-feab7908c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "  year(articleDate) as PublishingYear,\n",
    "  count(distinct title) as Articles\n",
    "from\n",
    "  example.wikipages\n",
    "where\n",
    "  month(articleDate)=3\n",
    "and\n",
    "  day(articleDate)=4\n",
    "group by\n",
    "  year(articleDate)\n",
    "order by\n",
    "  publishingYear\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc5d3f-0368-4c97-970b-7de8eea01fd7",
   "metadata": {},
   "source": [
    "## Bloom Filters\n",
    "\n",
    "Here we use the `countDistinct` function to get a number for the distinct items we want to index then expand it by an additional 25% to allow for growth.\n",
    "\n",
    "Afterwords we define the index for the same table from above.\n",
    "\n",
    "Recall that this would only index new or rewritten files for the table so anything already existing will not get indexed if we just completed the above.\n",
    "\n",
    "Instead we could place this action prior to our optimize action which then because of the rewrite would index all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85a6e6-1efd-4714-b171-3421feee0056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "cdf = spark.table(\"example.wikipages\")\n",
    "raw_items = cdf.agg(countDistinct(cdf.id)).collect()[0][0]\n",
    "num_items = int(raw_items * 1.25)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "create bloomfilter index\n",
    "on table\n",
    "example.wikipages\n",
    "for columns\n",
    "(id options (fpp=0.05, numItems={num_items}))\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
