{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17156fd5-da02-43de-96e9-eedd144d4b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 6: Maintaining your Delta Lake\n",
    "> The following exercises use the New York Times [Covid-19 NYT Dataset](https://github.com/delta-io/delta-docs/tree/main/static/quickstart_docker/rs/data/COVID-19_NYT).\n",
    "\n",
    "The dataset can be found in the `delta_quickstart` docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea01058-0674-4231-9e2d-bbcf10f81bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, desc, to_date\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cfece-8b8d-45f5-9abc-4d0ae3fa43ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.covid_nyt (\n",
    "  date DATE\n",
    ") USING DELTA\n",
    "TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c59705-37a5-4acb-9e85-7bfcc422cd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691f12a-e961-4f6a-bed3-b404ab4ffad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# will be empty on the first run. this is expected\n",
    "len(spark.table(\"default.covid_nyt\").inputFiles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4e08-d4b4-4471-bbc8-07aba57dd1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment if you'd like to begin again\n",
    "#spark.sql(\"drop table default.covid_nyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d37d79-4184-4d8f-b1a9-25fe08463ab6",
   "metadata": {},
   "source": [
    "## Start Populating the Table\n",
    "> The next three commands are used to show Schema Evolution and Validation with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2088c-f84f-4973-ab71-9051a710d31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate the Table reading the Parquet covid_nyc Data\n",
    "# note: this will fail on the first run, and that is okay\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae71d8-3659-437e-afca-b4cfc265e822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one step closer, there is still something missing...\n",
    "# and yes, this operation still fails... if only...\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061db2c-fc2e-4674-8e48-e62e06902d13",
   "metadata": {},
   "source": [
    "## Schema Evolution: Handle Automatically\n",
    "If you trust the upstream data source (provider) then you can add the `option(\"mergeSchema\", \"true\")`. Otherwise, it is better to specifically select a subset of the columns you expected to see. In this example use case, the only known column is `date`, so it is fairly safe to power ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1e23-48be-45e1-b627-9894991e53e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evolve the Schema. (Showcases how to auto-merge changes to the schema)\n",
    "# note: if you can trust the upstream, then this option is perfectly fine\n",
    "# however, if you don't trust the upstream, then it is good to opt-in to the \n",
    "# changing columns.\n",
    "\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"default.covid_nyt\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da844a5-7999-43d2-ad6e-76a0e2b5396f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"default.covid_nyt\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4931837-6282-4ac3-96f9-6febd15b96c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternatives to Auto Schema Evolution\n",
    "In the previous case, we used `.option(\"mergeSchema\", \"true\")` to modify the behavior of the Delta Lake writer. While this option simplifies how we evolve our Delta Lake table schemas, it comes at the price of not being fully aware of the changes to our table schema. In the case where there are unknown columns being introduced from an upstream source, you'll want to know which columns are intended to bring forward, and which columns can be safely ignored.\n",
    "\n",
    "## Intentionally Adding Columns with Alter Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723603f6-48bd-424b-96b5-750828a0f3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manually set the columns. This is an example of intentional opt-in to the new columns outside of '.option(\"mergeSchema\", \"true\")`. \n",
    "# Note: this can be run once, afterwards the ADD columns will fail since they already exist\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.covid_nyt \n",
    "ADD columns (\n",
    "  county STRING,\n",
    "  state STRING,\n",
    "  fips INT,\n",
    "  cases INT,\n",
    "  deaths INT\n",
    ");\n",
    "\"\"\")\n",
    "# notice how we are only using `.mode(\"append\")` and explicitly add `.option(\"mergeSchema\", \"false\")`. \n",
    "# this is how we stop unwanted columns from being freely added to our Delta Lake tables. It comes at the cost of raising exceptions and failing the job.\n",
    "# a failed job might seem like a bad option, but it is the cheaper option since you are intentionally blocking unknown data from flowing into your tables. \n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .option(\"mergeSchema\", \"false\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87216c69-b742-408f-a6fb-ccac96246b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf64e2-e0ea-4aeb-bdf5-352891a2cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from default.covid_nyt limit 10\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d76385-bf01-4079-93cc-6690d06a3a09",
   "metadata": {},
   "source": [
    "# Adding and Modifying Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a15b5-6b13-4e75-8faf-aa3b93c36b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'catalog.team_name'='dldg_authors',\n",
    "    'catalog.engineering.comms.slack'='https://delta-users.slack.com/archives/CG9LR6LN4',\n",
    "    'catalog.engineering.comms.email'='dldg_authors@gmail.com',\n",
    "    'catalog.table.classification'='all-access'\n",
    "  )\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d24afa-7fa7-4ee0-bdbd-94ff01f71e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table history\n",
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, 'default.covid_nyt')\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e0cda-e0ac-4920-8d37-1faaacbf8dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use DeltaTable to view\n",
    "dt.detail().select(\"properties\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a5c66-5183-43f7-8b79-3dfddae8d486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table properties\n",
    "spark.sql(\"show tblproperties default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b9a32-3cb9-425f-a925-70aa6faafd51",
   "metadata": {},
   "source": [
    "## Removing Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0dfad-60a5-4dfd-a20d-b4a10888cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add incorrect table property\n",
    "# which is blocked by default\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"true\")\n",
    "# now we can make a mistake\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'delta.loRgetentionDuratio'='interval 7 days'\n",
    "  )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a27b6-6b8d-475d-a357-8dfd1d2ae46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# luckily, we can remove the unwanted table property using UNSET\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  UNSET TBLPROPERTIES ('delta.loRgetentionDuratio')\n",
    "\"\"\")\n",
    "# now that we are done, let's just add back the safe guard again\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f5c63-f40a-490a-a22d-740ee85b10c8",
   "metadata": {},
   "source": [
    "## Delta Table Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56ac8e-77df-4d26-95a3-131b744228f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Creating the Small File Problem\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.nonoptimal_covid_nyt\")\n",
    "    .property(\"description\", \"table to be optimized\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", \"DATE\")\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8ee4a-4628-411d-8379-6707d67605dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table default.nonoptimal_covid_nyt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6155308-2f74-4097-868f-fd3c026c6895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can remove `repartition(9000)` and add write...option('maxRecordsPerFile`, 10000)\n",
    "# to generate more files using the DataFrameWriter\n",
    "(spark\n",
    "   .table(\"default.covid_nyt\")\n",
    "   .repartition(9000)\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   #.option(\"maxRecordsPerFile\", 1000)\n",
    "   .saveAsTable(\"default.nonoptimal_covid_nyt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f9428-c1b9-4c9b-a359-f4433d09b58c",
   "metadata": {},
   "source": [
    "## Using Optimize to Fix the Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f742e24-531c-4a22-aefa-0647493a2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the maxFileSize to a bin-size for optimize\n",
    "spark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", 1024*1024*1024)\n",
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .optimize()\n",
    "    .executeCompaction()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b9463-d642-45fc-ae83-ca4c17ad00ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Viewing the results of Optimize\n",
    "from pyspark.sql.functions import col\n",
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .history(10)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bd28d-4b45-424d-a3a9-afeff7186c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rewind and try again\n",
    "# note: the table version of the OPTIMIZE operation needs to be referenced to take the prior version\n",
    "#(DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").restoreToVersion(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff55f5-8d55-42b4-b9e2-289b7f3fdf54",
   "metadata": {},
   "source": [
    "## Partitioning, Repartitioning, and Default Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39850430-2c93-4812-8742-be1cb33ab51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import DateType\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.covid_nyt_by_date\")\n",
    "    .property(\"description\", \"table with default partitions\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", DateType(), nullable=False)\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .partitionedBy(\"date\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a2d0e-dd94-402d-8e43-df2a696c47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"drop table default.covid_nyt_by_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da1e9a-f9c5-4fda-8926-f9f671a3d6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use our non-partitioned source table to populate our partitioned table (automatically)\n",
    "(\n",
    "    spark\n",
    "    .table(\"default.covid_nyt\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"false\")\n",
    "    .saveAsTable(\"default.covid_nyt_by_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10adcd-27a2-418c-bf63-0b2a49956710",
   "metadata": {},
   "source": [
    "## Viewing the Partition Metadata of our Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9df68c-a8a0-4036-bb6a-96874a8dea72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt_by_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49de1f-eddf-4812-9b61-56f76c8973c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table metadata as a json blob\n",
    "\n",
    "DeltaTable.forName(spark, \"default.covid_nyt_by_date\").detail().toJSON().collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc472291-a327-4f26-8f47-997ef20b5bbb",
   "metadata": {},
   "source": [
    "# Create Bronze and Silver Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f689b-6bd0-487f-806e-a19cdfe1dae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952dd29-ced9-4c50-a825-f6460b560a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to first create two databases (schemas) in our Hive metastore, or Unity Catalog.\n",
    "# If using Unity Catalog, you can prefix <catalog>.<schema>.<table>\n",
    "# With Hive, you can only use <schema>.<table>\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967846a-3ecd-4994-baa7-b578920e05be",
   "metadata": {},
   "source": [
    "## COPY (CLONE) Tables between Databases (Schemas)\n",
    "> We will be copying `default.covid_nyt_by_date` using DEEP CLONE into `bronze.covid_nyt_by_date` and `silver.covid_nyt_by_date`\n",
    "> This functionality is available in the Databricks runtime as [CLONE](https://docs.databricks.com/delta/clone.html). [Shallow Cloning](https://docs.delta.io/latest/delta-utility.html#shallow-clone-a-delta-table) is available at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d27fa4-c603-4e55-8665-c2553a60923b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# slim version of https://github.com/MrPowers/mack/blob/main/mack/__init__.py#L288\n",
    "def copy_table(delta_table: DeltaTable, target_table: str):\n",
    "    details = (\n",
    "        delta_table\n",
    "        .detail()\n",
    "        .select(\"partitionColumns\", \"properties\")\n",
    "        .collect()[0]\n",
    "    )\n",
    "    (\n",
    "        table_to_copy.toDF().write.format(\"delta\")\n",
    "        .partitionBy(details[\"partitionColumns\"])\n",
    "        .options(**details[\"properties\"])\n",
    "        .saveAsTable(target_table)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43d80a-9a3c-4022-a98a-5d3be6a2f0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the default table and write into both bronze and silver\n",
    "table_to_copy = DeltaTable.forName(spark, \"default.covid_nyt_by_date\")\n",
    "bronze_table = \"bronze.covid_nyt_by_date\"\n",
    "silver_table = \"silver.covid_nyt_by_date\"\n",
    "\n",
    "copy_table(table_to_copy, bronze_table)\n",
    "copy_table(table_to_copy, silver_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72410b39-a731-44ee-911d-1c35b935aa03",
   "metadata": {},
   "source": [
    "## Using Shallow Clone to Create a Metadata-Only Copy of a Table\n",
    "Reference Link: https://docs.delta.io/latest/delta-utility.html#shallow-clone-a-delta-table\n",
    "\n",
    "The next example is extra content outside of the book materials for chapter 6. We'll discover how to shallow clone a table using both the path on disk, as well as from table to table references (for managed tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee57ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Shallow Clone to Create a Metadata Only Copy of the Table using the table location on disk\n",
    "src_location = DeltaTable.forName(spark, \"default.covid_nyt_by_date\").detail().first()[\"location\"]\n",
    "dest_location = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\").detail().first()[\"location\"]\n",
    "#print(f\"source_table:{src_location}\\ndestination_table_location:{dest_location}\")\n",
    "\n",
    "src_location_fmt = str(src_location).replace(\"file:\", \"\")\n",
    "# steal the silver.db location from the copy table, and just add _clone to the tablename\n",
    "dest_location_clone = str(dest_location).replace(\"file:\",\"\")+'_clone'\n",
    "#print(f\"src:{src_location_fmt}, dest:{dest_location_clone}\")\n",
    "\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS delta.`{dest_location_clone}` SHALLOW CLONE delta.`{dest_location_clone}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfefa4-e140-4ecd-86c2-5a2eeaef3678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"silver\")\n",
    "spark.catalog.listTables()\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "# On the first pass, without writing to the managed table location, you won't be able to see the new cloned table in the table\n",
    "# list. This is one way to work with cloned data where you are not \"broadcasting\" the table into the managed table space. When you are ready\n",
    "# you can always create a managed table using the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf435c-6cf8-4552-8b80-716a2866fa43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It is worth noting that you can CREATE a managed table over an existing non-managed table.\n",
    "# Observe the WARNING when running the next statement.\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.covid_nyt_by_date_clone SHALLOW CLONE default.covid_nyt_by_date\")\n",
    "\n",
    "# if you try to replace a CLONED table, you will get an exception \n",
    "# (DeltaIllegalStateException): The clone destination table is non-empty: Please TRUNCATE or DELETE before running CLONE...\n",
    "# this is to protect the integrity of the clone, the expectation for a SHALLOW CLONE is that it provides metadata only changes\n",
    "# as the source table is still the reference for the data.\n",
    "\n",
    "# To see the behavior in action, try\n",
    "# spark.sql(\"CREATE OR REPLACE TABLE silver.covid_nyt_by_date_clone SHALLOW CLONE default.covid_nyt_by_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339c2bf-e596-4e61-9f41-ae30a0a64b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after replacing the table clone, you'll see the table in the local table list\n",
    "spark.catalog.setCurrentDatabase(\"silver\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b67a5-fc59-4658-82d4-b99f0a278471",
   "metadata": {},
   "source": [
    "## Removing Partitions using Conditional Delete at the Partition Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1f788-5c98-486a-a0df-279f07b122c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Remove a partition from the silver table so we can repair the table with our bronze table\n",
    "silver_dt = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\")\n",
    "silver_dt.delete(col(\"date\") == \"2021-02-17\")\n",
    "\n",
    "# Note: (if you delete, and then immediately vacuum, you will not be able to restore your table)\n",
    "# vacuum to remove the physical data from the table\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "#silver_dt.vacuum(retentionHours=0)\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aeb1de-c78d-468c-b799-2ca813bda31d",
   "metadata": {},
   "source": [
    "## Using ReplaceWhere to do Conditional Repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4aa8993-c330-48c1-a75b-5d43ce3f2f82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/16 21:10:20 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/06/16 21:10:20 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/06/16 21:10:20 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/06/16 21:10:20 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "recovery_table = spark.table(\"bronze.covid_nyt_by_date\")\n",
    "partition_col = \"date\"\n",
    "table_to_fix = \"silver.covid_nyt_by_date\"\n",
    "\n",
    "(recovery_table.where(col(\"date\") == \"2021-02-17\").write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"replaceWhere\", f\"{partition_col} == '2021-02-17'\")\n",
    " .saveAsTable(\"silver.covid_nyt_by_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adf03d-6c18-4941-804a-8be6dbdc51bb",
   "metadata": {},
   "source": [
    "## Restoring Tables to a Prior Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5cdc9-7a1d-4496-b332-acc42d46fff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\")\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()\n",
    "dt.restoreToVersion(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af3f71-a249-4fea-885c-6bc4c896dd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning up our Delta Tables using Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20a7e6-ce3c-4074-b6aa-5f7b2372bfda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").vacuum(retentionHours=0)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b8f1b-6b46-49f2-9678-dd3bf56c0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select distinct(date) as date from default.covid_nyt_by_date order by date desc\").show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c6d12-78ce-4da3-8bf3-057bdae41275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(distinct(date)) from default.covid_nyt_by_date\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
