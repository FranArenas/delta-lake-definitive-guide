{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17156fd5-da02-43de-96e9-eedd144d4b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 6: Maintaining your Delta Lake\n",
    "> The following exercises use the New York Times [Covid-19 NYT Dataset](https://github.com/delta-io/delta-docs/tree/main/static/quickstart_docker/rs/data/COVID-19_NYT).\n",
    "\n",
    "The dataset can be found in the `delta_quickstart` docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea01058-0674-4231-9e2d-bbcf10f81bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, desc, to_date\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cfece-8b8d-45f5-9abc-4d0ae3fa43ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.covid_nyt (\n",
    "  date DATE\n",
    ") USING DELTA\n",
    "TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c59705-37a5-4acb-9e85-7bfcc422cd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691f12a-e961-4f6a-bed3-b404ab4ffad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# will be empty on the first run. this is expected\n",
    "len(spark.table(\"default.covid_nyt\").inputFiles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4e08-d4b4-4471-bbc8-07aba57dd1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment if you'd like to begin again\n",
    "#spark.sql(\"drop table default.covid_nyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d37d79-4184-4d8f-b1a9-25fe08463ab6",
   "metadata": {},
   "source": [
    "## Start Populating the Table\n",
    "> The next three commands are used to show Schema Evolution and Validation with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2088c-f84f-4973-ab71-9051a710d31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate the Table reading the Parquet covid_nyc Data\n",
    "# note: this will fail on the first run, and that is okay\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae71d8-3659-437e-afca-b4cfc265e822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one step closer, there is still something missing...\n",
    "# and yes, this operation still fails... if only...\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061db2c-fc2e-4674-8e48-e62e06902d13",
   "metadata": {},
   "source": [
    "## Schema Evolution: Handle Automatically\n",
    "If you trust the upstream data source (provider) then you can add the `option(\"mergeSchema\", \"true\")`. Otherwise, it is better to specifically select a subset of the columns you expected to see. In this example use case, the only known column is `date`, so it is fairly safe to power ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1e23-48be-45e1-b627-9894991e53e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evolve the Schema. (Showcases how to auto-merge changes to the schema)\n",
    "# note: if you can trust the upstream, then this option is perfectly fine\n",
    "# however, if you don't trust the upstream, then it is good to opt-in to the \n",
    "# changing columns.\n",
    "\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"default.covid_nyt\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da844a5-7999-43d2-ad6e-76a0e2b5396f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"default.covid_nyt\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4931837-6282-4ac3-96f9-6febd15b96c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternatives to Auto Schema Evolution\n",
    "In the previous case, we used `.option(\"mergeSchema\", \"true\")` to modify the behavior of the Delta Lake writer. While this option simplifies how we evolve our Delta Lake table schemas, it comes at the price of not being fully aware of the changes to our table schema. In the case where there are unknown columns being introduced from an upstream source, you'll want to know which columns are intended to bring forward, and which columns can be safely ignored.\n",
    "\n",
    "## Intentionally Adding Columns with Alter Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723603f6-48bd-424b-96b5-750828a0f3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manually set the columns. This is an example of intentional opt-in to the new columns outside of '.option(\"mergeSchema\", \"true\")`. \n",
    "# Note: this can be run once, afterwards the ADD columns will fail since they already exist\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.covid_nyt \n",
    "ADD columns (\n",
    "  county STRING,\n",
    "  state STRING,\n",
    "  fips INT,\n",
    "  cases INT,\n",
    "  deaths INT\n",
    ");\n",
    "\"\"\")\n",
    "# notice how we are only using `.mode(\"append\")` and explicitly add `.option(\"mergeSchema\", \"false\")`. \n",
    "# this is how we stop unwanted columns from being freely added to our Delta Lake tables. It comes at the cost of raising exceptions and failing the job.\n",
    "# a failed job might seem like a bad option, but it is the cheaper option since you are intentionally blocking unknown data from flowing into your tables. \n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .option(\"mergeSchema\", \"false\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87216c69-b742-408f-a6fb-ccac96246b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63bf64e2-e0ea-4aeb-bdf5-352891a2cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+-----+------+\n",
      "|      date|    county|   state| fips|cases|deaths|\n",
      "+----------+----------+--------+-----+-----+------+\n",
      "|2020-05-19|  Lawrence|Illinois|17101|    4|     0|\n",
      "|2020-05-19|       Lee|Illinois|17103|   75|     1|\n",
      "|2020-05-19|Livingston|Illinois|17105|   27|     1|\n",
      "|2020-05-19|     Logan|Illinois|17107|   10|     0|\n",
      "|2020-05-19|     Macon|Illinois|17115|  170|    17|\n",
      "|2020-05-19|  Macoupin|Illinois|17117|   42|     1|\n",
      "|2020-05-19|   Madison|Illinois|17119|  499|    45|\n",
      "|2020-05-19|    Marion|Illinois|17121|   49|     0|\n",
      "|2020-05-19|  Marshall|Illinois|17123|    5|     0|\n",
      "|2020-05-19|     Mason|Illinois|17125|   16|     0|\n",
      "+----------+----------+--------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.covid_nyt limit 10\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d76385-bf01-4079-93cc-6690d06a3a09",
   "metadata": {},
   "source": [
    "# Adding and Modifying Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff1a15b5-6b13-4e75-8faf-aa3b93c36b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'catalog.team_name'='dldg_authors',\n",
    "    'catalog.engineering.comms.slack'='https://delta-users.slack.com/archives/CG9LR6LN4',\n",
    "    'catalog.engineering.comms.email'='dldg_authors@gmail.com',\n",
    "    'catalog.table.classification'='all-access'\n",
    "  )\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d24afa-7fa7-4ee0-bdbd-94ff01f71e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table history\n",
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, 'default.covid_nyt')\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e0cda-e0ac-4920-8d37-1faaacbf8dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use DeltaTable to view\n",
    "dt.detail().select(\"properties\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a5c66-5183-43f7-8b79-3dfddae8d486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table properties\n",
    "spark.sql(\"show tblproperties default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b9a32-3cb9-425f-a925-70aa6faafd51",
   "metadata": {},
   "source": [
    "## Removing Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0dfad-60a5-4dfd-a20d-b4a10888cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add incorrect table property\n",
    "# which is blocked by default\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"true\")\n",
    "# now we can make a mistake\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'delta.loRgetentionDuratio'='interval 7 days'\n",
    "  )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b2a27b6-6b8d-475d-a357-8dfd1d2ae46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# luckily, we can remove the unwanted table property using UNSET\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  UNSET TBLPROPERTIES ('delta.loRgetentionDuratio')\n",
    "\"\"\")\n",
    "# now that we are done, let's just add back the safe guard again\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f5c63-f40a-490a-a22d-740ee85b10c8",
   "metadata": {},
   "source": [
    "## Delta Table Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56ac8e-77df-4d26-95a3-131b744228f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Creating the Small File Problem\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.nonoptimal_covid_nyt\")\n",
    "    .property(\"description\", \"table to be optimized\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", \"DATE\")\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8ee4a-4628-411d-8379-6707d67605dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table default.nonoptimal_covid_nyt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6155308-2f74-4097-868f-fd3c026c6895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can remove `repartition(9000)` and add write...option('maxRecordsPerFile`, 10000)\n",
    "# to generate more files using the DataFrameWriter\n",
    "(spark\n",
    "   .table(\"default.covid_nyt\")\n",
    "   .repartition(9000)\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   #.option(\"maxRecordsPerFile\", 1000)\n",
    "   .saveAsTable(\"default.nonoptimal_covid_nyt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f9428-c1b9-4c9b-a359-f4433d09b58c",
   "metadata": {},
   "source": [
    "## Using Optimize to Fix the Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f742e24-531c-4a22-aefa-0647493a2315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/08 23:57:55 WARN TaskSetManager: Stage 24 contains a task of very large size (1584 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the maxFileSize to a bin-size for optimize\n",
    "spark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", 1024*1024*1024)\n",
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .optimize()\n",
    "    .executeCompaction()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9b9463-d642-45fc-ae83-ca4c17ad00ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "|version|timestamp              |operation|numRemovedFiles|numAddedFiles|\n",
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "|3      |2023-06-08 23:58:34.667|OPTIMIZE |9000           |1            |\n",
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the results of Optimize\n",
    "from pyspark.sql.functions import col\n",
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .history(10)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bd28d-4b45-424d-a3a9-afeff7186c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rewind and try again\n",
    "# note: the table version of the OPTIMIZE operation needs to be referenced to take the prior version\n",
    "#(DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").restoreToVersion(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff55f5-8d55-42b4-b9e2-289b7f3fdf54",
   "metadata": {},
   "source": [
    "## Partitioning, Repartitioning, and Default Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39850430-2c93-4812-8742-be1cb33ab51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import DateType\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.covid_nyt_by_date\")\n",
    "    .property(\"description\", \"table with default partitions\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", DateType(), nullable=False)\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .partitionedBy(\"date\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a2d0e-dd94-402d-8e43-df2a696c47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"drop table default.covid_nyt_by_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da1e9a-f9c5-4fda-8926-f9f671a3d6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use our non-partitioned source table to populate our partitioned table (automatically)\n",
    "(\n",
    "    spark\n",
    "    .table(\"default.covid_nyt\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"false\")\n",
    "    .saveAsTable(\"default.covid_nyt_by_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10adcd-27a2-418c-bf63-0b2a49956710",
   "metadata": {},
   "source": [
    "## Viewing the Partition Metadata of our Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b9df68c-a8a0-4036-bb6a-96874a8dea72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                date|                date|       |\n",
      "|              county|              string|       |\n",
      "|               state|              string|       |\n",
      "|                fips|                 int|       |\n",
      "|               cases|                 int|       |\n",
      "|              deaths|                 int|       |\n",
      "|                    |                    |       |\n",
      "|      # Partitioning|                    |       |\n",
      "|              Part 0|                date|       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|default.covid_nyt...|       |\n",
      "|            Location|file:/opt/spark/w...|       |\n",
      "|            Provider|               delta|       |\n",
      "|               Owner|              NBuser|       |\n",
      "|    Table Properties|[catalog.engineer...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt_by_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3a49de1f-eddf-4812-9b61-56f76c8973c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"format\":\"delta\",\"id\":\"8c57bc67-369f-4c84-a63e-38b8ac19bdf2\",\"name\":\"default.covid_nyt_by_date\",\"location\":\"file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt_by_date\",\"createdAt\":\"2023-06-08T05:35:00.072Z\",\"lastModified\":\"2023-06-08T05:50:45.241Z\",\"partitionColumns\":[\"date\"],\"numFiles\":423,\"sizeInBytes\":17660304,\"properties\":{\"description\":\"table with default partitions\",\"catalog.table.classification\":\"all-access\",\"catalog.engineering.comms.email\":\"dldg_authors@gmail.com\",\"catalog.team_name\":\"dldg_authors\",\"catalog.engineering.comms.slack\":\"https://delta-users.slack.com/archives/CG9LR6LN4\"},\"minReaderVersion\":1,\"minWriterVersion\":2,\"tableFeatures\":[\"appendOnly\",\"invariants\"]}'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the table metadata as a json blob\n",
    "\n",
    "DeltaTable.forName(spark, \"default.covid_nyt_by_date\").detail().toJSON().collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc472291-a327-4f26-8f47-997ef20b5bbb",
   "metadata": {},
   "source": [
    "# Create Bronze and Silver Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f689b-6bd0-487f-806e-a19cdfe1dae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952dd29-ced9-4c50-a825-f6460b560a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to first create two databases (schemas) in our Hive metastore, or Unity Catalog.\n",
    "# If using Unity Catalog, you can prefix <catalog>.<schema>.<table>\n",
    "# With Hive, you can only use <schema>.<table>\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967846a-3ecd-4994-baa7-b578920e05be",
   "metadata": {},
   "source": [
    "## COPY (CLONE) Tables between Databases (Schemas)\n",
    "> We will be copying `default.covid_nyt_by_date` using DEEP CLONE into `bronze.covid_nyt_by_date` and `silver.covid_nyt_by_date`\n",
    "> This functionality is available in the Databricks runtime as [CLONE](https://docs.databricks.com/delta/clone.html). Cloning is on the [roadmap](https://github.com/delta-io/delta/issues/1307) but not currently in the OSS Delta project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d27fa4-c603-4e55-8665-c2553a60923b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# slim version of https://github.com/MrPowers/mack/blob/main/mack/__init__.py#L288\n",
    "def copy_table(delta_table: DeltaTable, target_table: str):\n",
    "    details = (\n",
    "        delta_table\n",
    "        .detail()\n",
    "        .select(\"partitionColumns\", \"properties\")\n",
    "        .collect()[0]\n",
    "    )\n",
    "    (\n",
    "        table_to_copy.toDF().write.format(\"delta\")\n",
    "        .partitionBy(details[\"partitionColumns\"])\n",
    "        .options(**details[\"properties\"])\n",
    "        .saveAsTable(target_table)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43d80a-9a3c-4022-a98a-5d3be6a2f0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the default table and write into both bronze and silver\n",
    "table_to_copy = DeltaTable.forName(spark, \"default.covid_nyt_by_date\")\n",
    "bronze_table = \"bronze.covid_nyt_by_date\"\n",
    "silver_table = \"silver.covid_nyt_by_date\"\n",
    "\n",
    "copy_table(table_to_copy, bronze_table)\n",
    "copy_table(table_to_copy, silver_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61cfefa4-e140-4ecd-86c2-5a2eeaef3678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c1f788-5c98-486a-a0df-279f07b122c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Remove a partition from the silver table so we can repair the table with our bronze table\n",
    "silver_dt = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\")\n",
    "silver_dt.delete(col(\"date\") == \"2021-02-17\")\n",
    "\n",
    "# Note: (if you delete, and then immediately vacuum, you will not be able to restore your table)\n",
    "# vacuum to remove the physical data from the table\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "#silver_dt.vacuum(retentionHours=0)\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aeb1de-c78d-468c-b799-2ca813bda31d",
   "metadata": {},
   "source": [
    "## Using ReplaceWhere to do Conditional Repairs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fc0ed15-0864-4d0c-a3de-b953dcd926f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "recovery_table = spark.table(\"bronze.covid_nyt_by_date\")\n",
    "partition_col = \"date\"\n",
    "table_to_fix = \"silver.covid_nyt_by_date\"\n",
    "\n",
    "(recovery_table.where(col(\"date\") == \"2021-02-17\").write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"replaceWhere\", f\"{partition_col} == '2021-02-17'\")\n",
    " .saveAsTable(\"silver.covid_nyt_by_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adf03d-6c18-4941-804a-8be6dbdc51bb",
   "metadata": {},
   "source": [
    "## Restoring Tables to a Prior Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecd5cdc9-7a1d-4496-b332-acc42d46fff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|version|           timestamp|           operation|\n",
      "+-------+--------------------+--------------------+\n",
      "|      4|2023-06-09 19:26:...|               WRITE|\n",
      "|      3|2023-06-09 19:17:...|          VACUUM END|\n",
      "|      2|2023-06-09 19:17:...|        VACUUM START|\n",
      "|      1|2023-06-09 19:11:...|              DELETE|\n",
      "|      0|2023-06-09 19:04:...|CREATE TABLE AS S...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\")\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()\n",
    "dt.restoreToVersion(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af3f71-a249-4fea-885c-6bc4c896dd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning up our Delta Tables using Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc20a7e6-ce3c-4074-b6aa-5f7b2372bfda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 9000 files and directories in a total of 1 directories.\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").vacuum(retentionHours=0)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e65b8f1b-6b46-49f2-9678-dd3bf56c0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-03-11|\n",
      "|2021-03-10|\n",
      "|2021-03-09|\n",
      "|2021-03-08|\n",
      "|2021-03-07|\n",
      "|2021-03-06|\n",
      "|2021-03-05|\n",
      "|2021-03-04|\n",
      "|2021-03-03|\n",
      "|2021-03-02|\n",
      "|2021-03-01|\n",
      "|2021-02-28|\n",
      "|2021-02-27|\n",
      "|2021-02-26|\n",
      "|2021-02-25|\n",
      "|2021-02-24|\n",
      "|2021-02-23|\n",
      "|2021-02-22|\n",
      "|2021-02-21|\n",
      "|2021-02-20|\n",
      "|2021-02-19|\n",
      "|2021-02-18|\n",
      "|2021-02-17|\n",
      "|2021-02-16|\n",
      "|2021-02-15|\n",
      "|2021-02-14|\n",
      "|2021-02-13|\n",
      "|2021-02-12|\n",
      "|2021-02-11|\n",
      "|2021-02-10|\n",
      "|2021-02-09|\n",
      "|2021-02-08|\n",
      "|2021-02-07|\n",
      "|2021-02-06|\n",
      "|2021-02-05|\n",
      "|2021-02-04|\n",
      "|2021-02-03|\n",
      "|2021-02-02|\n",
      "|2021-02-01|\n",
      "|2021-01-31|\n",
      "|2021-01-30|\n",
      "|2021-01-29|\n",
      "|2021-01-28|\n",
      "|2021-01-27|\n",
      "|2021-01-26|\n",
      "|2021-01-25|\n",
      "|2021-01-24|\n",
      "|2021-01-23|\n",
      "|2021-01-22|\n",
      "|2021-01-21|\n",
      "|2021-01-20|\n",
      "|2021-01-19|\n",
      "|2021-01-18|\n",
      "|2021-01-17|\n",
      "|2021-01-16|\n",
      "|2021-01-15|\n",
      "|2021-01-14|\n",
      "|2021-01-13|\n",
      "|2021-01-12|\n",
      "|2021-01-11|\n",
      "|2021-01-10|\n",
      "|2021-01-09|\n",
      "|2021-01-08|\n",
      "|2021-01-07|\n",
      "|2021-01-06|\n",
      "|2021-01-05|\n",
      "|2021-01-04|\n",
      "|2021-01-03|\n",
      "|2021-01-02|\n",
      "|2021-01-01|\n",
      "|2020-12-31|\n",
      "|2020-12-30|\n",
      "|2020-12-29|\n",
      "|2020-12-28|\n",
      "|2020-12-27|\n",
      "|2020-12-26|\n",
      "|2020-12-25|\n",
      "|2020-12-24|\n",
      "|2020-12-23|\n",
      "|2020-12-22|\n",
      "|2020-12-21|\n",
      "|2020-12-20|\n",
      "|2020-12-19|\n",
      "|2020-12-18|\n",
      "|2020-12-17|\n",
      "|2020-12-16|\n",
      "|2020-12-15|\n",
      "|2020-12-14|\n",
      "|2020-12-13|\n",
      "|2020-12-12|\n",
      "|2020-12-11|\n",
      "|2020-12-10|\n",
      "|2020-12-09|\n",
      "|2020-12-08|\n",
      "|2020-12-07|\n",
      "|2020-12-06|\n",
      "|2020-12-05|\n",
      "|2020-12-04|\n",
      "|2020-12-03|\n",
      "|2020-12-02|\n",
      "|2020-12-01|\n",
      "|2020-11-30|\n",
      "|2020-11-29|\n",
      "|2020-11-28|\n",
      "|2020-11-27|\n",
      "|2020-11-26|\n",
      "|2020-11-25|\n",
      "|2020-11-24|\n",
      "|2020-11-23|\n",
      "|2020-11-22|\n",
      "|2020-11-21|\n",
      "|2020-11-20|\n",
      "|2020-11-19|\n",
      "|2020-11-18|\n",
      "|2020-11-17|\n",
      "|2020-11-16|\n",
      "|2020-11-15|\n",
      "|2020-11-14|\n",
      "|2020-11-13|\n",
      "|2020-11-12|\n",
      "|2020-11-11|\n",
      "|2020-11-10|\n",
      "|2020-11-09|\n",
      "|2020-11-08|\n",
      "|2020-11-07|\n",
      "|2020-11-06|\n",
      "|2020-11-05|\n",
      "|2020-11-04|\n",
      "|2020-11-03|\n",
      "|2020-11-02|\n",
      "|2020-11-01|\n",
      "|2020-10-31|\n",
      "|2020-10-30|\n",
      "|2020-10-29|\n",
      "|2020-10-28|\n",
      "|2020-10-27|\n",
      "|2020-10-26|\n",
      "|2020-10-25|\n",
      "|2020-10-24|\n",
      "|2020-10-23|\n",
      "|2020-10-22|\n",
      "|2020-10-21|\n",
      "|2020-10-20|\n",
      "|2020-10-19|\n",
      "|2020-10-18|\n",
      "|2020-10-17|\n",
      "|2020-10-16|\n",
      "|2020-10-15|\n",
      "|2020-10-14|\n",
      "|2020-10-13|\n",
      "|2020-10-12|\n",
      "|2020-10-11|\n",
      "|2020-10-10|\n",
      "|2020-10-09|\n",
      "|2020-10-08|\n",
      "|2020-10-07|\n",
      "|2020-10-06|\n",
      "|2020-10-05|\n",
      "|2020-10-04|\n",
      "|2020-10-03|\n",
      "|2020-10-02|\n",
      "|2020-10-01|\n",
      "|2020-09-30|\n",
      "|2020-09-29|\n",
      "|2020-09-28|\n",
      "|2020-09-27|\n",
      "|2020-09-26|\n",
      "|2020-09-25|\n",
      "|2020-09-24|\n",
      "|2020-09-23|\n",
      "|2020-09-22|\n",
      "|2020-09-21|\n",
      "|2020-09-20|\n",
      "|2020-09-19|\n",
      "|2020-09-18|\n",
      "|2020-09-17|\n",
      "|2020-09-16|\n",
      "|2020-09-15|\n",
      "|2020-09-14|\n",
      "|2020-09-13|\n",
      "|2020-09-12|\n",
      "|2020-09-11|\n",
      "|2020-09-10|\n",
      "|2020-09-09|\n",
      "|2020-09-08|\n",
      "|2020-09-07|\n",
      "|2020-09-06|\n",
      "|2020-09-05|\n",
      "|2020-09-04|\n",
      "|2020-09-03|\n",
      "|2020-09-02|\n",
      "|2020-09-01|\n",
      "|2020-08-31|\n",
      "|2020-08-30|\n",
      "|2020-08-29|\n",
      "|2020-08-28|\n",
      "|2020-08-27|\n",
      "|2020-08-26|\n",
      "|2020-08-25|\n",
      "|2020-08-24|\n",
      "+----------+\n",
      "only showing top 200 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(date) as date from default.covid_nyt_by_date order by date desc\").show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544c6d12-78ce-4da3-8bf3-057bdae41275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/08 23:57:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/06/08 23:57:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/06/08 23:57:13 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/06/08 23:57:13 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.17.0.2\n",
      "23/06/08 23:57:13 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/06/08 23:57:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:========>                                                 (2 + 8) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT date)|\n",
      "+--------------------+\n",
      "|                 416|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(date)) from default.covid_nyt_by_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31036c-44b8-495d-99c6-840b36caf795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
