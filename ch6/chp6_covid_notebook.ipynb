{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17156fd5-da02-43de-96e9-eedd144d4b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 6: Maintaining your Delta Lake\n",
    "> The following exercises use the New York Times [Covid-19 NYT Dataset](https://github.com/delta-io/delta-docs/tree/main/static/quickstart_docker/rs/data/COVID-19_NYT).\n",
    "\n",
    "The dataset can be found in the `delta_quickstart` docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea01058-0674-4231-9e2d-bbcf10f81bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, desc, to_date\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cfece-8b8d-45f5-9abc-4d0ae3fa43ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.covid_nyt (\n",
    "  date DATE\n",
    ") USING DELTA\n",
    "TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c59705-37a5-4acb-9e85-7bfcc422cd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691f12a-e961-4f6a-bed3-b404ab4ffad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# will be empty on the first run. this is expected\n",
    "len(spark.table(\"default.covid_nyt\").inputFiles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4e08-d4b4-4471-bbc8-07aba57dd1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment if you'd like to begin again\n",
    "#spark.sql(\"drop table default.covid_nyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d37d79-4184-4d8f-b1a9-25fe08463ab6",
   "metadata": {},
   "source": [
    "## Start Populating the Table\n",
    "> The next three commands are used to show Schema Evolution and Validation with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2088c-f84f-4973-ab71-9051a710d31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate the Table reading the Parquet covid_nyc Data\n",
    "# note: this will fail on the first run, and that is okay\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae71d8-3659-437e-afca-b4cfc265e822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one step closer, there is still something missing...\n",
    "# and yes, this operation still fails... if only...\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061db2c-fc2e-4674-8e48-e62e06902d13",
   "metadata": {},
   "source": [
    "## Schema Evolution: Handle Automatically\n",
    "If you trust the upstream data source (provider) then you can add the `option(\"mergeSchema\", \"true\")`. Otherwise, it is better to specifically select a subset of the columns you expected to see. In this example use case, the only known column is `date`, so it is fairly safe to power ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1e23-48be-45e1-b627-9894991e53e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evolve the Schema. (Showcases how to auto-merge changes to the schema)\n",
    "# note: if you can trust the upstream, then this option is perfectly fine\n",
    "# however, if you don't trust the upstream, then it is good to opt-in to the \n",
    "# changing columns.\n",
    "\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"default.covid_nyt\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da844a5-7999-43d2-ad6e-76a0e2b5396f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"default.covid_nyt\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4931837-6282-4ac3-96f9-6febd15b96c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternatives to Auto Schema Evolution\n",
    "In the previous case, we used `.option(\"mergeSchema\", \"true\")` to modify the behavior of the Delta Lake writer. While this option simplifies how we evolve our Delta Lake table schemas, it comes at the price of not being fully aware of the changes to our table schema. In the case where there are unknown columns being introduced from an upstream source, you'll want to know which columns are intended to bring forward, and which columns can be safely ignored.\n",
    "\n",
    "## Intentionally Adding Columns with Alter Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723603f6-48bd-424b-96b5-750828a0f3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manually set the columns. This is an example of intentional opt-in to the new columns outside of '.option(\"mergeSchema\", \"true\")`. \n",
    "# Note: this can be run once, afterwards the ADD columns will fail since they already exist\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.covid_nyt \n",
    "ADD columns (\n",
    "  county STRING,\n",
    "  state STRING,\n",
    "  fips INT,\n",
    "  cases INT,\n",
    "  deaths INT\n",
    ");\n",
    "\"\"\")\n",
    "# notice how we are only using `.mode(\"append\")` and explicitly add `.option(\"mergeSchema\", \"false\")`. \n",
    "# this is how we stop unwanted columns from being freely added to our Delta Lake tables. It comes at the cost of raising exceptions and failing the job.\n",
    "# a failed job might seem like a bad option, but it is the cheaper option since you are intentionally blocking unknown data from flowing into your tables. \n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .option(\"mergeSchema\", \"false\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87216c69-b742-408f-a6fb-ccac96246b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                     |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "|date                        |date                                                                                          |       |\n",
      "|county                      |string                                                                                        |       |\n",
      "|state                       |string                                                                                        |       |\n",
      "|fips                        |int                                                                                           |       |\n",
      "|cases                       |int                                                                                           |       |\n",
      "|deaths                      |int                                                                                           |       |\n",
      "|                            |                                                                                              |       |\n",
      "|# Partitioning              |                                                                                              |       |\n",
      "|Not partitioned             |                                                                                              |       |\n",
      "|                            |                                                                                              |       |\n",
      "|# Detailed Table Information|                                                                                              |       |\n",
      "|Name                        |default.covid_nyt                                                                             |       |\n",
      "|Location                    |file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt                                        |       |\n",
      "|Provider                    |delta                                                                                         |       |\n",
      "|Owner                       |NBuser                                                                                        |       |\n",
      "|Table Properties            |[delta.logRetentionDuration=interval 7 days,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63bf64e2-e0ea-4aeb-bdf5-352891a2cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+-----+------+\n",
      "|      date|    county|   state| fips|cases|deaths|\n",
      "+----------+----------+--------+-----+-----+------+\n",
      "|2020-05-19|  Lawrence|Illinois|17101|    4|     0|\n",
      "|2020-05-19|       Lee|Illinois|17103|   75|     1|\n",
      "|2020-05-19|Livingston|Illinois|17105|   27|     1|\n",
      "|2020-05-19|     Logan|Illinois|17107|   10|     0|\n",
      "|2020-05-19|     Macon|Illinois|17115|  170|    17|\n",
      "|2020-05-19|  Macoupin|Illinois|17117|   42|     1|\n",
      "|2020-05-19|   Madison|Illinois|17119|  499|    45|\n",
      "|2020-05-19|    Marion|Illinois|17121|   49|     0|\n",
      "|2020-05-19|  Marshall|Illinois|17123|    5|     0|\n",
      "|2020-05-19|     Mason|Illinois|17125|   16|     0|\n",
      "+----------+----------+--------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.covid_nyt limit 10\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d76385-bf01-4079-93cc-6690d06a3a09",
   "metadata": {},
   "source": [
    "# Adding and Modifying Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff1a15b5-6b13-4e75-8faf-aa3b93c36b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'catalog.team_name'='dldg_authors',\n",
    "    'catalog.engineering.comms.slack'='https://delta-users.slack.com/archives/CG9LR6LN4',\n",
    "    'catalog.engineering.comms.email'='dldg_authors@gmail.com',\n",
    "    'catalog.table.classification'='all-access'\n",
    "  )\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d24afa-7fa7-4ee0-bdbd-94ff01f71e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table history\n",
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, 'default.covid_nyt')\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e0cda-e0ac-4920-8d37-1faaacbf8dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use DeltaTable to view\n",
    "dt.detail().select(\"properties\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a5c66-5183-43f7-8b79-3dfddae8d486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table properties\n",
    "spark.sql(\"show tblproperties default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b9a32-3cb9-425f-a925-70aa6faafd51",
   "metadata": {},
   "source": [
    "## Removing Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0dfad-60a5-4dfd-a20d-b4a10888cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add incorrect table property\n",
    "# which is blocked by default\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"true\")\n",
    "# now we can make a mistake\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'delta.loRgetentionDuratio'='interval 7 days'\n",
    "  )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b2a27b6-6b8d-475d-a357-8dfd1d2ae46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# luckily, we can remove the unwanted table property using UNSET\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  UNSET TBLPROPERTIES ('delta.loRgetentionDuratio')\n",
    "\"\"\")\n",
    "# now that we are done, let's just add back the safe guard again\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f5c63-f40a-490a-a22d-740ee85b10c8",
   "metadata": {},
   "source": [
    "## Delta Table Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56ac8e-77df-4d26-95a3-131b744228f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Creating the Small File Problem\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.nonoptimal_covid_nyt\")\n",
    "    .property(\"description\", \"table to be optimized\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", \"DATE\")\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8ee4a-4628-411d-8379-6707d67605dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table default.nonoptimal_covid_nyt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6155308-2f74-4097-868f-fd3c026c6895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(spark\n",
    "   .table(\"default.covid_nyt\")\n",
    "   .repartition(9000)\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(\"default.nonoptimal_covid_nyt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f9428-c1b9-4c9b-a359-f4433d09b58c",
   "metadata": {},
   "source": [
    "## Using Optimize to Fix the Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3f742e24-531c-4a22-aefa-0647493a2315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/07 06:46:52 WARN TaskSetManager: Stage 618 contains a task of very large size (1584 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .optimize()\n",
    "    .executeCompaction()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af3f71-a249-4fea-885c-6bc4c896dd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning up our Delta Tables using Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc20a7e6-ce3c-4074-b6aa-5f7b2372bfda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 9000 files and directories in a total of 1 directories.\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").vacuum(retentionHours=0)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b8f1b-6b46-49f2-9678-dd3bf56c0e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
