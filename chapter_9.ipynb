{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a44baa",
   "metadata": {},
   "source": [
    "# Chapter 9: Streaming in and out of your Delta Lake\n",
    "\n",
    "first draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba51011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset(s)\n",
    "# Define spark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8353bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta readStream example\n",
    "\n",
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .load(\"/files/delta/user_events\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta writeStream example\n",
    "\n",
    "(streamingDeltaDf\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(“append”)\n",
    ".start(\"/<delta_path>/\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta chained readStream to writeStream\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"ignoreChanges\", \"true\")\n",
    ".load(\"/files/delta/user_events\")\n",
    "# …\n",
    "# <other transformation logic>\n",
    "# …\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(“append”)\n",
    ".start(\"/<delta_path>/\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a59155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting ignoreDeletes\n",
    "\n",
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .load(\"/files/delta/user_events\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5deb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting ignoreChanges\n",
    "\n",
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreChanges\", \"true\")\n",
    "    .load(\"/files/delta/user_events\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7975f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the starting version\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"startingVersion\", \"5\")\n",
    ".load(\"/files/delta/user_events\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90631dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the starting timestamp\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"startingTimestamp\", \"2023-04-18\")\n",
    ".load(\"/files/delta/user_events\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting eventTimeOrder with a watermark\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"withEventTimeOrder\", \"true\")\n",
    ".load(\"/files/delta/user_events\")\n",
    ".withWatermark(\"event_time\", \"10 seconds\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d534d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a forEachBatch function using txnVersion options for idempotency\n",
    "\n",
    "app_id = ... # A unique string used as an application ID.\n",
    "\n",
    "def writeToDeltaLakeTableIdempotent(batch_df, batch_id):\n",
    "    # location 1\n",
    "    (batch_df\n",
    "    .write\n",
    "    .format(“delta”)\n",
    "    .option(\"txnVersion\", batch_id)\n",
    "    .option(\"txnAppId\", app_id)\n",
    "    .save(\"/<delta_path>/\")\n",
    "    )\n",
    "    # location 2\n",
    "    (batch_df\n",
    "    .write\n",
    "    .format(“delta”)\n",
    "    .option(\"txnVersion\", batch_id)\n",
    "    .option(\"txnAppId\", app_id)\n",
    "    .save(\"/<delta_path>/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Delta Lake mergeBuilder to create an upsert forEachBatch function and apply to a writeStream\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "changesStream = ... # Streaming dataframe with CDC records\n",
    "\n",
    "# Function to upsert microBatchDf into Delta table using merge\n",
    "def upsertToDelta(microBatchDf, batchId):\n",
    "    \"\"\"Use Delta APIs to handle merge logic into table\"\"\"\n",
    "    deltaTable = DeltaTable.forName(spark, \"retail_db.transactions_silver\") # Target table\n",
    "\n",
    "    deltaTable.alias(\"dt\") \\\n",
    "    .merge(\n",
    "        source = microBatchDf.alias(\"sdf\"),\n",
    "        condition = \"sdf.t_id = dt.t_id\"\n",
    "        ) \\\n",
    "    .whenMatchedDelete(condition = \"sdf.operation = 'DELETE'\") \\\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"t_id\": \"sdf.t_id\",\n",
    "        \"transaction_date\": \"sdf.transaction_date\",\n",
    "        \"item_count\": \"sdf.item_count\",\n",
    "        \"amount\": \"sdf.amount\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"t_id\": \"sdf.t_id\",\n",
    "        \"transaction_date\": \"sdf.transaction_date\",\n",
    "        \"item_count\": \"sdf.item_count\",\n",
    "        \"amount\": \"sdf.amount\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table\n",
    "(changesStream\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".queryName(\"Summaries Silver Pipeline\")\n",
    ".foreachBatch(upsertToDelta)\n",
    ".outputMode(\"update\")\n",
    ".start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2365c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Live Tables (DLT) syntactical example\n",
    "\n",
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def autoloader_dlt_bronze():\n",
    "    return (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(\"<data path>\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def delta_dlt_silver():\n",
    "    return (\n",
    "        dlt\n",
    "        .read_stream(\"autoloader_dlt_bronze\")\n",
    "        …\n",
    "        <transformation logic>\n",
    "        …\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def live_delta_gold():\n",
    "    return (\n",
    "        dlt\n",
    "        .read(\"delta_dlt_silver\")\n",
    "        …\n",
    "        <aggregation logic>\n",
    "        …\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Change Data Feed read boundaries in a batch process\n",
    "\n",
    "# Specify the version as int or long\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .option(\"endingVersion\", 10) \\\n",
    "  .table(\"myDeltaTable\")\n",
    "\n",
    "# Specify timestamps as formatted timestamp\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", '2023-04-01 05:45:46') \\\n",
    "  .option(\"endingTimestamp\", '2023-04-21 12:00:00') \\\n",
    "  .table(\"myDeltaTable\")\n",
    "\n",
    "# Providing only the startingVersion/timestamp\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", '2023-04-21 12:00:00.001') \\\n",
    "  .table(\"myDeltaTable\")\n",
    "\n",
    "\n",
    "# Specifying similarly with a file location\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n",
    "  .load(\"/pathToMyDeltaTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6310bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Change Data Feed read boundaries in a batch process\n",
    "\n",
    "# Specifying a starting version\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .load(\"/pathToMyDeltaTable\")\n",
    "\n",
    "# Specifying a starting timestamp\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n",
    "  .load(\"/pathToMyDeltaTable\")\n",
    "\n",
    "# Not providing either option, i.e., process from beginning\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .load(\"/pathToMyDeltaTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for viewing changes (needs updating)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    people10m\n",
    "        SET\n",
    "            gender = 'F',\n",
    "            firstName='Leah'\n",
    "        WHERE\n",
    "            firstName='Leo'\n",
    "            and lastName='Conkay';\n",
    "    \"\"\")\n",
    "\n",
    "(spark\n",
    ".read\n",
    ".format(\"delta\")\n",
    ".option(\"readChangeFeed\", \"true\")\n",
    ".option(\"startingVersion\", 5)\n",
    ".option(\"endingVersion\", 5)\n",
    ".table(\"tristen.people10m\")\n",
    ".select(\n",
    "    col(\"firstName\"),\n",
    "    col(\"lastName\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"_change_type\"),\n",
    "    col(\"_commit_version\"))\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
