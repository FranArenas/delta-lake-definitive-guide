{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a44baa",
   "metadata": {},
   "source": [
    "# Chapter 7: Streaming in and out of your Delta Lake\n",
    "\n",
    "draft: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92771793",
   "metadata": {},
   "source": [
    "# Notebook Intro and Setup\n",
    "\n",
    "This notebook should run end to end with the \"run all\" mode.\n",
    "\n",
    "We encourage exploration and breaking stuff to get the most from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba51011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create some resources\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "try:\n",
    "    os.chdir(\"/opt/spark/work-dir/ch9/\")\n",
    "except FileNotFoundError:\n",
    "    raise\n",
    "\n",
    "# Remove any old checkpoints\n",
    "try:\n",
    "    subprocess.run([\"rm\", \"-rf\", f\"./ckpt/\"])\n",
    "except:\n",
    "    raise\n",
    "\n",
    "# Set some Spark configuration options\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# set COVID dataset path\n",
    "covid_data_path = '/opt/spark/work-dir/rs/data/COVID-19_NYT'\n",
    "\n",
    "# Kill any other active streams\n",
    "for stream in spark.streams.active:\n",
    "    stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e5115",
   "metadata": {},
   "source": [
    "# Delta `readStream` example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8353bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .load(covid_data_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4fe33",
   "metadata": {},
   "source": [
    "# Delta `writeStream` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0276c8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(streamingDeltaDf\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointLocation\", \"./delta/ckpt/ws1/\")\n",
    ".start(\"./delta/covid/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c5b6f2-6b74-41aa-a999-1b33c1c6b89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43137b0c",
   "metadata": {},
   "source": [
    "# Delta chained `readStream` to `writeStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92306015-4a44-4cbb-91ad-88e5bdcbfd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111930"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the count of the table\n",
    "(spark\n",
    ".read\n",
    ".format(\"delta\")\n",
    ".load(\"./delta/covid/\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6508b4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".load(\"./delta/covid/\")\n",
    "# in the book this says '<other transformation logic>'\n",
    ".filter(col(\"deaths\") > 0)\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointLocation\", \"./delta/ckpt/ws2/\")\n",
    ".start(\"./delta/covid_deaths/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7588eb2d-e1c0-4a5f-ad76-7173a796960a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bb572",
   "metadata": {},
   "source": [
    "# Setting `ignoreDeletes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a59155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .load(\"./delta/covid_deaths/\")\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a119ed",
   "metadata": {},
   "source": [
    "# Setting `ignoreChanges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5deb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreChanges\", \"true\")\n",
    "    .load(\"./delta/covid_deaths/\")\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbdf60",
   "metadata": {},
   "source": [
    "# Specify the `startingVersion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7975f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"startingVersion\", \"5\")\n",
    ".load(\"./delta/covid_deaths/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ef405",
   "metadata": {},
   "source": [
    "# Specify the `startingTimestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90631dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"startingTimestamp\", \"2023-04-18\")\n",
    ".load(\"./delta/covid_deaths/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485124b",
   "metadata": {},
   "source": [
    "# Setting `eventTimeOrder` with a watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfff98ae-6c1e-4d1d-8003-5d75183979cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".load(\"./delta/covid/\")\n",
    ".withColumn(\"event_time\", current_timestamp())\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointLocation\", \"./delta/ckpt/ws3/\")\n",
    ".start(\"./delta/covid_deaths_with_ts/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ee1c281-0d9b-4ff2-b8fb-9ff81d503cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e404bffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"withEventTimeOrder\", \"true\")\n",
    ".load(\"./delta/covid_deaths_with_ts/\")\n",
    ".withWatermark(\"event_time\", \"10 seconds\")\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointLocation\", \"./delta/ckpt/ws4/\")\n",
    ".start(\"./delta/covid_deaths_with_ts2/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d019d",
   "metadata": {},
   "source": [
    "# Idempotent Fanout\n",
    "\n",
    "Example of a `forEachBatch` function using `txnVersion` options for idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8b0c257-47aa-466e-8f75-5d224d732504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a new table, enable change data feed, get some change records going\n",
    "(spark\n",
    ".read\n",
    ".format(\"delta\")\n",
    ".load(\"./delta/covid/\")\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".saveAsTable(\"covid_table\")\n",
    ")\n",
    "spark.sql(\"ALTER TABLE covid_table SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "start_ts = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "for i in range(10):\n",
    "    (spark\n",
    "    .read\n",
    "    .format(\"delta\")\n",
    "    .load(covid_data_path)\n",
    "    .sample(0.05)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"covid_table\")\n",
    "    );\n",
    "    if i == 7:\n",
    "        end_ts = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# We'll need a second table too\n",
    "(spark\n",
    ".read\n",
    ".format(\"delta\")\n",
    ".load(\"./delta/covid/\")\n",
    ".sample(0.01)\n",
    ".drop_duplicates([\"fips\", \"date\", \"county\", \"state\"])\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".saveAsTable(\"covid_table2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef3a1e95-2b1c-4965-bffc-6a7b87ea938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d534d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_id = \"delta_idempotency_example\" # A unique string used as an application ID.\n",
    "\n",
    "def writeToDeltaLakeTableIdempotent(batch_df, batch_id):\n",
    "    # location 1\n",
    "    (batch_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .option(\"txnVersion\", batch_id)\n",
    "    .option(\"txnAppId\", app_id)\n",
    "    .save(\"./delta/idempotent_location_1/\")\n",
    "    )\n",
    "    # location 2\n",
    "    (batch_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .option(\"txnVersion\", batch_id)\n",
    "    .option(\"txnAppId\", app_id)\n",
    "    .save(\"./delta/idempotent_location_2/\")\n",
    "    )\n",
    "\n",
    "    \n",
    "changesStream = ( # Streaming dataframe with CDC records\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", 1)\n",
    "    .table(\"covid_table\")\n",
    "    )\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table\n",
    "(changesStream\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".queryName(\"A Pipeline\")\n",
    ".foreachBatch(writeToDeltaLakeTableIdempotent)\n",
    ".outputMode(\"update\")\n",
    ".start()\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282c009",
   "metadata": {},
   "source": [
    "# Streaming Upsert\n",
    "\n",
    "Using the Delta Lake `mergeBuilder` to create an upsert `forEachBatch` function and apply to a `writeStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5791221b-ebf5-4585-b9d5-0f47497796a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fips: integer (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- _change_type: string (nullable = true)\n",
      " |-- _commit_version: long (nullable = true)\n",
      " |-- _commit_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.table(\"covid_table\").printSchema()\n",
    "changesStream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26cb866b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3a78138cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "changesStream = ( # Streaming dataframe with CDC records\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", 1)\n",
    "    .table(\"covid_table\")\n",
    "    )\n",
    "\n",
    "# Function to upsert microBatchDf into Delta table using merge\n",
    "def upsertToDelta(microBatchDf, batchId):\n",
    "    \"\"\"Use Delta APIs to handle merge logic into table\"\"\"\n",
    "    deltaTable = DeltaTable.forName(spark, \"covid_table2\") # Target table\n",
    "\n",
    "    deltaTable.alias(\"dt\") \\\n",
    "    .merge(\n",
    "        source = microBatchDf.alias(\"sdf\").drop_duplicates([\"fips\", \"date\", \"county\", \"state\"]),\n",
    "        condition = \"sdf.fips = dt.fips and sdf.date = dt.date and sdf.county = dt.county and sdf.state = dt.state\"\n",
    "        ) \\\n",
    "    .whenMatchedDelete(condition = \"sdf._change_type = 'DELETE'\") \\\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"fips\": \"sdf.fips\",\n",
    "        \"date\": \"sdf.date\",\n",
    "        \"cases\": \"sdf.cases\",\n",
    "        \"deaths\": \"sdf.deaths\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"fips\": \"sdf.fips\",\n",
    "        \"date\": \"sdf.date\",\n",
    "        \"cases\": \"sdf.cases\",\n",
    "        \"deaths\": \"sdf.deaths\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table\n",
    "(changesStream\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".queryName(\"New Pipeline\")\n",
    ".foreachBatch(upsertToDelta)\n",
    ".outputMode(\"update\")\n",
    ".start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c4686",
   "metadata": {},
   "source": [
    "# Delta Live Tables (DLT)\n",
    "\n",
    "A syntactical example, you'll have to go try out something like this on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279d9ae-abd6-4bdd-ad14-221041007aad",
   "metadata": {},
   "source": [
    "```\n",
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def autoloader_dlt_bronze():\n",
    "    return (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(\"<data path>\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def delta_dlt_silver():\n",
    "    return (\n",
    "        dlt\n",
    "        .read_stream(\"autoloader_dlt_bronze\")\n",
    "        …\n",
    "        <transformation logic>\n",
    "        …\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def live_delta_gold():\n",
    "    return (\n",
    "        dlt\n",
    "        .read(\"delta_dlt_silver\")\n",
    "        …\n",
    "        <aggregation logic>\n",
    "        …\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429c647",
   "metadata": {},
   "source": [
    "# Defining Change Data Feed read boundaries in a batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b3d2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-26T22:49:03 2023-07-26T22:49:10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, county: string, state: string, fips: int, cases: int, deaths: int, _change_type: string, _commit_version: bigint, _commit_timestamp: timestamp]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(start_ts, end_ts)\n",
    "\n",
    "# Specify the version as int or long\n",
    "(spark\n",
    "  .read\n",
    "  .format(\"delta\")\n",
    "  .option(\"readChangeFeed\", \"true\")\n",
    "  .option(\"startingVersion\", 1)\n",
    "  .option(\"endingVersion\", 10)\n",
    "  .table(\"covid_table\")\n",
    ")\n",
    "\n",
    "# Specify timestamps as formatted timestamp\n",
    "(spark\n",
    "  .read\n",
    "  .format(\"delta\")\n",
    "  .option(\"readChangeFeed\", \"true\")\n",
    "  .option(\"startingTimestamp\", start_ts)\n",
    "  .option(\"endingTimestamp\", end_ts)\n",
    "  .table(\"covid_table\")\n",
    ")\n",
    " \n",
    "# Providing only the startingVersion/timestamp\n",
    "(spark\n",
    "  .read\n",
    "  .format(\"delta\")\n",
    "  .option(\"readChangeFeed\", \"true\")\n",
    "  .option(\"startingTimestamp\", start_ts)\n",
    "  .table(\"covid_table\")\n",
    ")\n",
    "\n",
    "# Specifying similarly with a file location\n",
    "(spark\n",
    "  .read\n",
    "  .format(\"delta\")\n",
    "  .option(\"readChangeFeed\", \"true\")\n",
    "  .option(\"startingTimestamp\", start_ts)\n",
    "  .load(\"/opt/spark/work-dir/spark-warehouse/covid_table/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38ea1e",
   "metadata": {},
   "source": [
    "# Defining Change Data Feed read boundaries in a streaming process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6310bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, county: string, state: string, fips: int, cases: int, deaths: int, _change_type: string, _commit_version: bigint, _commit_timestamp: timestamp]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying a starting version\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 1) \\\n",
    "  .load(\"/opt/spark/work-dir/spark-warehouse/covid_table/\")\n",
    "\n",
    "# Specifying a starting timestamp\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", start_ts) \\\n",
    "  .load(\"/opt/spark/work-dir/spark-warehouse/covid_table/\")\n",
    "\n",
    "# Not providing either option, i.e., process from beginning\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .load(\"/opt/spark/work-dir/spark-warehouse/covid_table/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27c80d",
   "metadata": {},
   "source": [
    "# Example for viewing changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "184dd32e-5ab8-4117-a3f2-529a2d65c69e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/26 22:49:24 ERROR TorrentBroadcast: Store broadcast broadcast_204 fail, remove all pieces of the broadcast\n",
      "23/07/26 22:49:24 ERROR TorrentBroadcast: Store broadcast broadcast_205 fail, remove all pieces of the broadcast\n"
     ]
    }
   ],
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d4a1cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    update covid_table\n",
    "        SET\n",
    "            cases = 100,\n",
    "            deaths = 1\n",
    "        WHERE\n",
    "            date='2020-07-08'\n",
    "            and fips=21025;\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2dc2b5a-40bc-49ee-9098-cc0b474d0917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+------+----------------+---------------+\n",
      "|      date| fips|cases|deaths|    _change_type|_commit_version|\n",
      "+----------+-----+-----+------+----------------+---------------+\n",
      "|2020-07-08|21025|   12|     0| update_preimage|             12|\n",
      "|2020-07-08|21025|  100|     1|update_postimage|             12|\n",
      "+----------+-----+-----+------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    ".read\n",
    ".format(\"delta\")\n",
    ".option(\"readChangeFeed\", \"true\")\n",
    ".option(\"startingVersion\", 12)\n",
    ".table(\"covid_table\")\n",
    ".select(\n",
    "    col(\"date\"),\n",
    "    col(\"fips\"),\n",
    "    col(\"cases\"),\n",
    "    col(\"deaths\"),\n",
    "    col(\"_change_type\"),\n",
    "    col(\"_commit_version\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b7dc3-72e1-4bf4-9cb5-f24b9ebd140b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
