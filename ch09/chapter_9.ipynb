{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a44baa",
   "metadata": {},
   "source": [
    "# Chapter 9: Streaming in and out of your Delta Lake\n",
    "\n",
    "draft: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92771793",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba51011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create some resources\n",
    "import os\n",
    "os.makedirs(\"./delta/ch9/\")  # create working directory for this notebook\n",
    "os.makedirs(\"./delta/ch9/ckpt/\")  # create a common checkpointing directory \n",
    "\n",
    "# set COVID dataset path\n",
    "covid_data_path = '/opt/spark/work-dir/rs/data/COVID-19_NYT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e5115",
   "metadata": {},
   "source": [
    "# Delta `readStream` example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8353bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .load(covid_data_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4fe33",
   "metadata": {},
   "source": [
    "# Delta `writeStream` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0276c8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 12:33:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/05/19 12:33:26 WARN StreamingQueryManager: Stopping existing streaming query [id=304120fb-6798-450f-a033-a123559ec17b, runId=a5476195-bddf-4060-bad6-999f43cfab6e], as a new run is being started.\n"
     ]
    }
   ],
   "source": [
    "(streamingDeltaDf\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointLocation\", \".ckpt/ws1/\")\n",
    ".start(\"./covid/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43137b0c",
   "metadata": {},
   "source": [
    "# Delta chained `readStream` to `writeStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6508b4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 12:41:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff9ddc14040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 12:41:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "(spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".load(\"./covid/\")\n",
    "# in the book this says '<other transformation logic>'\n",
    ".filter(col(\"deaths\") > 0)\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointLocation\", \".ckpt/ws2/\")\n",
    ".start(\"./covid_deaths/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bb572",
   "metadata": {},
   "source": [
    "# Setting `ignoreDeletes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2a59155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .load(\"./covid_deaths/\")\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a119ed",
   "metadata": {},
   "source": [
    "# Setting `ignoreChanges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee5deb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"ignoreChanges\", \"true\")\n",
    "    .load(\"./covid_deaths/\")\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbdf60",
   "metadata": {},
   "source": [
    "# Specify the `startingVersion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7975f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"startingVersion\", \"5\")\n",
    ".load(\"./covid_deaths/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ef405",
   "metadata": {},
   "source": [
    "# Specify the `startingTimestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90631dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDeltaDf = (spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"startingTimestamp\", \"2023-04-18\")\n",
    ".load(\"./covid_deaths/\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485124b",
   "metadata": {},
   "source": [
    "# Setting `eventTimeOrder` with a watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e404bffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'event_time' does not exist. Did you mean one of the following? [state, county, date, deaths, fips, cases];\n'EventTimeWatermark 'event_time, 10 seconds\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3228627a,delta,List(),None,List(),None,Map(withEventTimeOrder -> true, path -> ./covid_deaths/),None), delta, [date#2554, county#2555, state#2556, fips#2557, cases#2558, deaths#2559]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m streamingDeltaDf \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwithEventTimeOrder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./covid_deaths/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:746\u001b[0m, in \u001b[0;36mDataFrame.withWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delayThreshold \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(delayThreshold) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelayThreshold should be provided as a string interval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 746\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayThreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'event_time' does not exist. Did you mean one of the following? [state, county, date, deaths, fips, cases];\n'EventTimeWatermark 'event_time, 10 seconds\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3228627a,delta,List(),None,List(),None,Map(withEventTimeOrder -> true, path -> ./covid_deaths/),None), delta, [date#2554, county#2555, state#2556, fips#2557, cases#2558, deaths#2559]\n"
     ]
    }
   ],
   "source": [
    "streamingDeltaDf = (spark\n",
    ".readStream\n",
    ".format(\"delta\")\n",
    ".option(\"withEventTimeOrder\", \"true\")\n",
    ".load(\"./covid_deaths/\")\n",
    ".withWatermark(\"event_time\", \"10 seconds\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d019d",
   "metadata": {},
   "source": [
    "# Idempotent Fanout\n",
    "\n",
    "Example of a `forEachBatch` function using `txnVersion` options for idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d534d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = ... # A unique string used as an application ID.\n",
    "\n",
    "def writeToDeltaLakeTableIdempotent(batch_df, batch_id):\n",
    "    # location 1\n",
    "    (batch_df\n",
    "    .write\n",
    "    .format(“delta”)\n",
    "    .option(\"txnVersion\", batch_id)\n",
    "    .option(\"txnAppId\", app_id)\n",
    "    .save(\"/<delta_path>/\")\n",
    "    )\n",
    "    # location 2\n",
    "    (batch_df\n",
    "    .write\n",
    "    .format(“delta”)\n",
    "    .option(\"txnVersion\", batch_id)\n",
    "    .option(\"txnAppId\", app_id)\n",
    "    .save(\"/<delta_path>/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282c009",
   "metadata": {},
   "source": [
    "# Streaming Upsert\n",
    "\n",
    "Using the Delta Lake `mergeBuilder` to create an upsert `forEachBatch` function and apply to a `writeStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "changesStream = ... # Streaming dataframe with CDC records\n",
    "\n",
    "# Function to upsert microBatchDf into Delta table using merge\n",
    "def upsertToDelta(microBatchDf, batchId):\n",
    "    \"\"\"Use Delta APIs to handle merge logic into table\"\"\"\n",
    "    deltaTable = DeltaTable.forName(spark, \"retail_db.transactions_silver\") # Target table\n",
    "\n",
    "    deltaTable.alias(\"dt\") \\\n",
    "    .merge(\n",
    "        source = microBatchDf.alias(\"sdf\"),\n",
    "        condition = \"sdf.t_id = dt.t_id\"\n",
    "        ) \\\n",
    "    .whenMatchedDelete(condition = \"sdf.operation = 'DELETE'\") \\\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"t_id\": \"sdf.t_id\",\n",
    "        \"transaction_date\": \"sdf.transaction_date\",\n",
    "        \"item_count\": \"sdf.item_count\",\n",
    "        \"amount\": \"sdf.amount\"\n",
    "        }) \\\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"t_id\": \"sdf.t_id\",\n",
    "        \"transaction_date\": \"sdf.transaction_date\",\n",
    "        \"item_count\": \"sdf.item_count\",\n",
    "        \"amount\": \"sdf.amount\"\n",
    "        }) \\\n",
    "    .execute()\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table\n",
    "(changesStream\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".queryName(\"Summaries Silver Pipeline\")\n",
    ".foreachBatch(upsertToDelta)\n",
    ".outputMode(\"update\")\n",
    ".start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c4686",
   "metadata": {},
   "source": [
    "# Delta Live Tables (DLT)\n",
    "\n",
    "A syntactical example, you'll have to go try out something like this on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2365c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def autoloader_dlt_bronze():\n",
    "    return (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(\"<data path>\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def delta_dlt_silver():\n",
    "    return (\n",
    "        dlt\n",
    "        .read_stream(\"autoloader_dlt_bronze\")\n",
    "        …\n",
    "        <transformation logic>\n",
    "        …\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def live_delta_gold():\n",
    "    return (\n",
    "        dlt\n",
    "        .read(\"delta_dlt_silver\")\n",
    "        …\n",
    "        <aggregation logic>\n",
    "        …\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429c647",
   "metadata": {},
   "source": [
    "# Defining Change Data Feed read boundaries in a batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the version as int or long\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .option(\"endingVersion\", 10) \\\n",
    "  .table(\"myDeltaTable\")\n",
    "\n",
    "# Specify timestamps as formatted timestamp\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", '2023-04-01 05:45:46') \\\n",
    "  .option(\"endingTimestamp\", '2023-04-21 12:00:00') \\\n",
    "  .table(\"myDeltaTable\")\n",
    "\n",
    "# Providing only the startingVersion/timestamp\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", '2023-04-21 12:00:00.001') \\\n",
    "  .table(\"myDeltaTable\")\n",
    "\n",
    "# Specifying similarly with a file location\n",
    "spark.read.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n",
    "  .load(\"/pathToMyDeltaTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38ea1e",
   "metadata": {},
   "source": [
    "# Defining Change Data Feed read boundaries in a streaming process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6310bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying a starting version\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingVersion\", 0) \\\n",
    "  .load(\"/pathToMyDeltaTable\")\n",
    "\n",
    "# Specifying a starting timestamp\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n",
    "  .load(\"/pathToMyDeltaTable\")\n",
    "\n",
    "# Not providing either option, i.e., process from beginning\n",
    "spark.readStream.format(\"delta\") \\\n",
    "  .option(\"readChangeFeed\", \"true\") \\\n",
    "  .load(\"/pathToMyDeltaTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27c80d",
   "metadata": {},
   "source": [
    "# Example for viewing changes (needs updating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    people10m\n",
    "        SET\n",
    "            gender = 'F',\n",
    "            firstName='Leah'\n",
    "        WHERE\n",
    "            firstName='Leo'\n",
    "            and lastName='Conkay';\n",
    "    \"\"\")\n",
    "\n",
    "(spark\n",
    ".read\n",
    ".format(\"delta\")\n",
    ".option(\"readChangeFeed\", \"true\")\n",
    ".option(\"startingVersion\", 5)\n",
    ".option(\"endingVersion\", 5)\n",
    ".table(\"tristen.people10m\")\n",
    ".select(\n",
    "    col(\"firstName\"),\n",
    "    col(\"lastName\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"_change_type\"),\n",
    "    col(\"_commit_version\"))\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
